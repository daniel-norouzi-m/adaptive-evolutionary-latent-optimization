{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import necessary modules \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from numpy.linalg import eigvalsh\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import itertools\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rbf_volatility_surface import RBFVolatilitySurface\n",
    "from smoothness_prior import RBFQuadraticSmoothnessPrior\n",
    "from dataset_sabr import generate_sabr_call_options\n",
    "from surface_vae_trainer import SurfaceVAETrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the strike price list and maturity time list\n",
    "strike_price_list = np.array([0.75, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.2, 1.3, 1.5])\n",
    "maturity_time_list = np.array([0.02, 0.08, 0.17, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 3.0])\n",
    "\n",
    "# Create the product grid of maturity times and strike prices\n",
    "product_grid = list(product(maturity_time_list, strike_price_list))\n",
    "maturity_times, strike_prices = zip(*product_grid)\n",
    "\n",
    "# Convert to arrays for further operations\n",
    "maturity_times = np.array(maturity_times)\n",
    "strike_prices = np.array(strike_prices)\n",
    "\n",
    "# Variance formula for log-uniform distribution\n",
    "def log_uniform_variance(a, b):\n",
    "    log_term = np.log(b / a)\n",
    "    var = ((b ** 2 - a ** 2) / (2 * log_term)) - ((b - a) / log_term) ** 2\n",
    "    return var\n",
    "\n",
    "# Calculate standard deviations for maturity times and strike prices\n",
    "maturity_std = np.sqrt(log_uniform_variance(maturity_time_list.min(), maturity_time_list.max()))\n",
    "strike_std = np.sqrt(log_uniform_variance(strike_price_list.min(), strike_price_list.max()))\n",
    "\n",
    "# Define the SABR model parameters\n",
    "alpha = 0.20  # Stochastic volatility parameter\n",
    "beta = 0.50   # Elasticity parameter\n",
    "rho = -0.75   # Correlation between asset price and volatility\n",
    "nu = 1.0      # Volatility of volatility parameter\n",
    "\n",
    "# Other model parameters\n",
    "risk_free_rate = np.log(1.02)  # Risk-free interest rate\n",
    "underlying_price = 1.0         # Current price of the underlying asset\n",
    "\n",
    "# Generate the dataset using the SABR model and Black-Scholes formula\n",
    "call_option_dataset = generate_sabr_call_options(\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    rho=rho,\n",
    "    nu=nu,\n",
    "    maturity_times=maturity_times,\n",
    "    strike_prices=strike_prices,\n",
    "    risk_free_rate=risk_free_rate,\n",
    "    underlying_price=underlying_price\n",
    ")\n",
    "\n",
    "# Maturity times and strike prices from the previous product grid setup\n",
    "hypothetical_maturity_time_list = np.logspace(np.log10(0.01), np.log10(3.1), 100)\n",
    "hypothetical_strike_price_list = np.logspace(np.log10(0.7), np.log10(1.75), 100)\n",
    "\n",
    "# Create the product grid of maturity times and strike prices\n",
    "hypothetical_product_grid = list(product(hypothetical_maturity_time_list, hypothetical_strike_price_list))\n",
    "hypothetical_maturity_times, hypothetical_strike_prices = zip(*hypothetical_product_grid)\n",
    "hypothetical_maturity_times, hypothetical_strike_prices = np.array(hypothetical_maturity_times), np.array(hypothetical_strike_prices)\n",
    "\n",
    "# Reshape the data for 3D surface plotting\n",
    "hypothetical_maturities_grid = hypothetical_maturity_times.reshape((len(hypothetical_maturity_time_list), len(hypothetical_strike_price_list)))  \n",
    "hypothetical_strikes_grid = hypothetical_strike_prices.reshape((len(hypothetical_maturity_time_list), len(hypothetical_strike_price_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_roots = 350\n",
    "# n_roots = 10\n",
    "smoothness_controller = 3.274549162877732e-05\n",
    "\n",
    "# Initialize the RBFQuadraticSmoothnessPrior class\n",
    "smoothness_prior = RBFQuadraticSmoothnessPrior(\n",
    "    maturity_times=maturity_times,\n",
    "    strike_prices=strike_prices,\n",
    "    maturity_std=maturity_std,\n",
    "    strike_std=strike_std,\n",
    "    n_roots=n_roots,\n",
    "    smoothness_controller=smoothness_controller,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "prior_covariance_matrix = smoothness_prior.prior_covariance()\n",
    "prior_eigenvalues = np.sort(np.linalg.eigvalsh(prior_covariance_matrix))[::-1].copy()\n",
    "\n",
    "# The constant_volatility is set to a reasonable value\n",
    "constant_volatility = RBFVolatilitySurface.calculate_constant_volatility(\n",
    "    call_option_dataset[\"Implied Volatility\"],\n",
    "    call_option_dataset[\"Time to Maturity\"],\n",
    "    call_option_dataset[\"Strike Price\"],\n",
    "    risk_free_rate,\n",
    "    underlying_price\n",
    ")\n",
    "\n",
    "sampled_surface_coefficients = smoothness_prior.sample_smooth_surfaces(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 70  # Latent dimension\n",
    "data_dim = 100  # Data dimension of input\n",
    "latent_diagonal = prior_eigenvalues[:latent_dim]  # Eigenvalues for latent prior\n",
    "batch_size = 1000  # Batch size for training\n",
    "beta_ = 1.0  # Beta value for beta-VAE\n",
    "fine_tune_learning_rate = 1e-4  # Fine-tune learning rate\n",
    "pre_train_epochs = 350  # Number of pre-train epochs\n",
    "fine_tune_epochs = 20  # Number of fine-tune epochs\n",
    "device = \"cpu\"  # Use CPU as the device\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "hidden_dim_grid = [128, 256, 512]  # Example grid for hidden_dim\n",
    "n_layers_grid = [2, 4, 8]         # Example grid for n_layers\n",
    "pre_train_learning_rate_grid = [1e-4, 1e-3, 1e-2]  # Example grid for learning rate\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Define the grid search\n",
    "grid = itertools.product(hidden_dim_grid, n_layers_grid, pre_train_learning_rate_grid)\n",
    "\n",
    "for hidden_dim, n_layers, pre_train_learning_rate in tqdm(grid):\n",
    "    # Initialize the trainer with the specified configuration\n",
    "    trainer = SurfaceVAETrainer(\n",
    "        latent_dim=latent_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        n_layers=n_layers,\n",
    "        data_dim=data_dim,\n",
    "        latent_diagonal=latent_diagonal,\n",
    "        batch_size=batch_size,\n",
    "        beta=beta_,\n",
    "        pre_train_learning_rate=pre_train_learning_rate,\n",
    "        fine_tune_learning_rate=fine_tune_learning_rate,\n",
    "        pre_train_epochs=pre_train_epochs,\n",
    "        fine_tune_epochs=fine_tune_epochs,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Train the model using pre_train\n",
    "    trainer.pre_train_with_sampling(\n",
    "        smoothness_prior=smoothness_prior,\n",
    "        experiment_name=f\"test_hd_{hidden_dim}_nl_{n_layers}_lr_{pre_train_learning_rate}\"\n",
    "    )\n",
    "\n",
    "    # Retrieve the last row of the loss history (assuming it's stored in trainer.pre_train_loss_history)\n",
    "    loss_df = pd.DataFrame(trainer.pre_train_loss_history)\n",
    "    last_row = loss_df.iloc[-1].copy()\n",
    "\n",
    "    # Add the configuration as columns in the last row\n",
    "    last_row['hidden_dim'] = hidden_dim\n",
    "    last_row['n_layers'] = n_layers\n",
    "    last_row['pre_train_learning_rate'] = pre_train_learning_rate\n",
    "\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([last_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reconstruction Loss</th>\n",
       "      <th>KL Loss</th>\n",
       "      <th>Total Loss</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>pre_train_learning_rate</th>\n",
       "      <th>average_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.050729</td>\n",
       "      <td>2.842956</td>\n",
       "      <td>2.893685</td>\n",
       "      <td>512.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.051739</td>\n",
       "      <td>0.690140</td>\n",
       "      <td>0.741879</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.051627</td>\n",
       "      <td>2.068588</td>\n",
       "      <td>2.120214</td>\n",
       "      <td>256.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.051970</td>\n",
       "      <td>0.259207</td>\n",
       "      <td>0.311176</td>\n",
       "      <td>512.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.050912</td>\n",
       "      <td>9.276610</td>\n",
       "      <td>9.327522</td>\n",
       "      <td>512.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.051512</td>\n",
       "      <td>3.765398</td>\n",
       "      <td>3.816909</td>\n",
       "      <td>256.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.051636</td>\n",
       "      <td>3.176954</td>\n",
       "      <td>3.228590</td>\n",
       "      <td>128.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.051809</td>\n",
       "      <td>3.002637</td>\n",
       "      <td>3.054446</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.051310</td>\n",
       "      <td>12.442724</td>\n",
       "      <td>12.494034</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.052277</td>\n",
       "      <td>0.581075</td>\n",
       "      <td>0.633352</td>\n",
       "      <td>512.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.052445</td>\n",
       "      <td>0.244185</td>\n",
       "      <td>0.296630</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.075422</td>\n",
       "      <td>0.014938</td>\n",
       "      <td>0.090360</td>\n",
       "      <td>256.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.062143</td>\n",
       "      <td>0.122372</td>\n",
       "      <td>0.184515</td>\n",
       "      <td>256.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.053372</td>\n",
       "      <td>1.408692</td>\n",
       "      <td>1.462064</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.085330</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>0.093414</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051976</td>\n",
       "      <td>8.833328</td>\n",
       "      <td>8.885304</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.051905</td>\n",
       "      <td>12.716245</td>\n",
       "      <td>12.768150</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.052230</td>\n",
       "      <td>28.074957</td>\n",
       "      <td>28.127188</td>\n",
       "      <td>128.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.051819</td>\n",
       "      <td>378.640320</td>\n",
       "      <td>378.692139</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.052355</td>\n",
       "      <td>17.694931</td>\n",
       "      <td>17.747286</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.057294</td>\n",
       "      <td>12.541232</td>\n",
       "      <td>12.598526</td>\n",
       "      <td>512.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.053610</td>\n",
       "      <td>32.613663</td>\n",
       "      <td>32.667274</td>\n",
       "      <td>256.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.053574</td>\n",
       "      <td>38.580193</td>\n",
       "      <td>38.633766</td>\n",
       "      <td>128.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.077729</td>\n",
       "      <td>201.308167</td>\n",
       "      <td>201.385895</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.106820</td>\n",
       "      <td>1628.038452</td>\n",
       "      <td>1628.145264</td>\n",
       "      <td>256.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.821048</td>\n",
       "      <td>7164.605957</td>\n",
       "      <td>7165.427246</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Reconstruction Loss      KL Loss   Total Loss  hidden_dim  n_layers  \\\n",
       "24             0.050729     2.842956     2.893685       512.0       8.0   \n",
       "16             0.051739     0.690140     0.741879       256.0       8.0   \n",
       "13             0.051627     2.068588     2.120214       256.0       4.0   \n",
       "25             0.051970     0.259207     0.311176       512.0       8.0   \n",
       "21             0.050912     9.276610     9.327522       512.0       4.0   \n",
       "10             0.051512     3.765398     3.816909       256.0       2.0   \n",
       "7              0.051636     3.176954     3.228590       128.0       8.0   \n",
       "19             0.051809     3.002637     3.054446       512.0       2.0   \n",
       "17             0.051310    12.442724    12.494034       256.0       8.0   \n",
       "22             0.052277     0.581075     0.633352       512.0       4.0   \n",
       "5              0.052445     0.244185     0.296630       128.0       4.0   \n",
       "11             0.075422     0.014938     0.090360       256.0       2.0   \n",
       "14             0.062143     0.122372     0.184515       256.0       4.0   \n",
       "2              0.053372     1.408692     1.462064       128.0       2.0   \n",
       "20             0.085330     0.008084     0.093414       512.0       2.0   \n",
       "1              0.051976     8.833328     8.885304       128.0       2.0   \n",
       "15             0.051905    12.716245    12.768150       256.0       8.0   \n",
       "8              0.052230    28.074957    28.127188       128.0       8.0   \n",
       "18             0.051819   378.640320   378.692139       512.0       2.0   \n",
       "4              0.052355    17.694931    17.747286       128.0       4.0   \n",
       "23             0.057294    12.541232    12.598526       512.0       4.0   \n",
       "12             0.053610    32.613663    32.667274       256.0       4.0   \n",
       "6              0.053574    38.580193    38.633766       128.0       8.0   \n",
       "3              0.077729   201.308167   201.385895       128.0       4.0   \n",
       "9              0.106820  1628.038452  1628.145264       256.0       2.0   \n",
       "0              0.821048  7164.605957  7165.427246       128.0       2.0   \n",
       "26                  NaN          NaN          NaN       512.0       8.0   \n",
       "\n",
       "    pre_train_learning_rate  average_rank  \n",
       "24                   0.0001           5.5  \n",
       "16                   0.0010           7.0  \n",
       "13                   0.0010           7.0  \n",
       "25                   0.0010           8.0  \n",
       "21                   0.0001           8.5  \n",
       "10                   0.0010           8.5  \n",
       "7                    0.0010           9.0  \n",
       "19                   0.0010           9.5  \n",
       "17                   0.0100           9.5  \n",
       "22                   0.0010          10.0  \n",
       "5                    0.0100          10.0  \n",
       "11                   0.0100          12.0  \n",
       "14                   0.0100          12.0  \n",
       "2                    0.0100          12.5  \n",
       "20                   0.0100          12.5  \n",
       "1                    0.0010          13.0  \n",
       "15                   0.0001          14.0  \n",
       "8                    0.0100          16.5  \n",
       "18                   0.0001          16.5  \n",
       "4                    0.0010          17.0  \n",
       "23                   0.0100          18.5  \n",
       "12                   0.0001          20.0  \n",
       "6                    0.0001          20.0  \n",
       "3                    0.0001          23.0  \n",
       "9                    0.0001          25.0  \n",
       "0                    0.0001          26.0  \n",
       "26                   0.0100           NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank the losses for each column (except 'Total Loss')\n",
    "ranked_losses = results_df.drop(columns=['Total Loss', 'hidden_dim', 'n_layers', 'pre_train_learning_rate']).rank()\n",
    "\n",
    "ranked_df = results_df.copy()\n",
    "\n",
    "# Compute the average rank for each configuration\n",
    "ranked_df['average_rank'] = ranked_losses.mean(axis=1)\n",
    "\n",
    "# Sort by the average rank (lower is better)\n",
    "ranked_df = ranked_df.sort_values('average_rank')\n",
    "\n",
    "# Print the top-ranked configurations\n",
    "ranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600, Batch 1, Losses: {'Reconstruction Loss': 6.999632358551025, 'KL Loss': 62261.0, 'Total Loss': 62268.0}\n",
      "Epoch 2/600, Batch 1, Losses: {'Reconstruction Loss': 3.442704200744629, 'KL Loss': 60993.34765625, 'Total Loss': 60996.7890625}\n",
      "Epoch 3/600, Batch 1, Losses: {'Reconstruction Loss': 1.7724372148513794, 'KL Loss': 52939.8046875, 'Total Loss': 52941.578125}\n",
      "Epoch 4/600, Batch 1, Losses: {'Reconstruction Loss': 0.9979664087295532, 'KL Loss': 34494.4453125, 'Total Loss': 34495.44140625}\n",
      "Epoch 5/600, Batch 1, Losses: {'Reconstruction Loss': 0.47238582372665405, 'KL Loss': 13564.9208984375, 'Total Loss': 13565.3935546875}\n",
      "Epoch 6/600, Batch 1, Losses: {'Reconstruction Loss': 0.5227526426315308, 'KL Loss': 12545.759765625, 'Total Loss': 12546.2822265625}\n",
      "Epoch 7/600, Batch 1, Losses: {'Reconstruction Loss': 1.4964783191680908, 'KL Loss': 12437.5712890625, 'Total Loss': 12439.0673828125}\n",
      "Epoch 8/600, Batch 1, Losses: {'Reconstruction Loss': 1.5469316244125366, 'KL Loss': 9895.7666015625, 'Total Loss': 9897.3134765625}\n",
      "Epoch 9/600, Batch 1, Losses: {'Reconstruction Loss': 1.8836926221847534, 'KL Loss': 6215.587890625, 'Total Loss': 6217.4716796875}\n",
      "Epoch 10/600, Batch 1, Losses: {'Reconstruction Loss': 1.7137385606765747, 'KL Loss': 5102.43017578125, 'Total Loss': 5104.14404296875}\n",
      "Epoch 11/600, Batch 1, Losses: {'Reconstruction Loss': 1.0780318975448608, 'KL Loss': 3793.649658203125, 'Total Loss': 3794.727783203125}\n",
      "Epoch 12/600, Batch 1, Losses: {'Reconstruction Loss': 1.5972115993499756, 'KL Loss': 2474.509521484375, 'Total Loss': 2476.106689453125}\n",
      "Epoch 13/600, Batch 1, Losses: {'Reconstruction Loss': 1.0208817720413208, 'KL Loss': 2405.423828125, 'Total Loss': 2406.44482421875}\n",
      "Epoch 14/600, Batch 1, Losses: {'Reconstruction Loss': 0.5973833799362183, 'KL Loss': 2966.138916015625, 'Total Loss': 2966.736328125}\n",
      "Epoch 15/600, Batch 1, Losses: {'Reconstruction Loss': 0.7869369983673096, 'KL Loss': 3176.881103515625, 'Total Loss': 3177.66796875}\n",
      "Epoch 16/600, Batch 1, Losses: {'Reconstruction Loss': 0.6849003434181213, 'KL Loss': 2827.08154296875, 'Total Loss': 2827.766357421875}\n",
      "Epoch 17/600, Batch 1, Losses: {'Reconstruction Loss': 0.3617017865180969, 'KL Loss': 2303.741943359375, 'Total Loss': 2304.103759765625}\n",
      "Epoch 18/600, Batch 1, Losses: {'Reconstruction Loss': 0.36649882793426514, 'KL Loss': 1675.875732421875, 'Total Loss': 1676.2421875}\n",
      "Epoch 19/600, Batch 1, Losses: {'Reconstruction Loss': 0.3102552592754364, 'KL Loss': 1098.319580078125, 'Total Loss': 1098.6298828125}\n",
      "Epoch 20/600, Batch 1, Losses: {'Reconstruction Loss': 0.5103885531425476, 'KL Loss': 858.0807495117188, 'Total Loss': 858.5911254882812}\n",
      "Epoch 21/600, Batch 1, Losses: {'Reconstruction Loss': 0.6276708245277405, 'KL Loss': 907.0927734375, 'Total Loss': 907.720458984375}\n",
      "Epoch 22/600, Batch 1, Losses: {'Reconstruction Loss': 0.45052650570869446, 'KL Loss': 967.9579467773438, 'Total Loss': 968.408447265625}\n",
      "Epoch 23/600, Batch 1, Losses: {'Reconstruction Loss': 0.23850463330745697, 'KL Loss': 863.4246826171875, 'Total Loss': 863.6632080078125}\n",
      "Epoch 24/600, Batch 1, Losses: {'Reconstruction Loss': 0.2557494342327118, 'KL Loss': 653.2066650390625, 'Total Loss': 653.46240234375}\n",
      "Epoch 25/600, Batch 1, Losses: {'Reconstruction Loss': 0.1814538836479187, 'KL Loss': 496.48138427734375, 'Total Loss': 496.662841796875}\n",
      "Epoch 26/600, Batch 1, Losses: {'Reconstruction Loss': 0.15063107013702393, 'KL Loss': 457.4636535644531, 'Total Loss': 457.6142883300781}\n",
      "Epoch 27/600, Batch 1, Losses: {'Reconstruction Loss': 0.20960748195648193, 'KL Loss': 451.411376953125, 'Total Loss': 451.6209716796875}\n",
      "Epoch 28/600, Batch 1, Losses: {'Reconstruction Loss': 0.41195279359817505, 'KL Loss': 428.1786804199219, 'Total Loss': 428.59063720703125}\n",
      "Epoch 29/600, Batch 1, Losses: {'Reconstruction Loss': 0.37263864278793335, 'KL Loss': 437.6556396484375, 'Total Loss': 438.0282897949219}\n",
      "Epoch 30/600, Batch 1, Losses: {'Reconstruction Loss': 0.1771387755870819, 'KL Loss': 409.9320068359375, 'Total Loss': 410.109130859375}\n",
      "Epoch 31/600, Batch 1, Losses: {'Reconstruction Loss': 0.22964543104171753, 'KL Loss': 303.21075439453125, 'Total Loss': 303.4403991699219}\n",
      "Epoch 32/600, Batch 1, Losses: {'Reconstruction Loss': 0.2903531491756439, 'KL Loss': 226.15231323242188, 'Total Loss': 226.4426727294922}\n",
      "Epoch 33/600, Batch 1, Losses: {'Reconstruction Loss': 0.3367882966995239, 'KL Loss': 230.84286499023438, 'Total Loss': 231.17965698242188}\n",
      "Epoch 34/600, Batch 1, Losses: {'Reconstruction Loss': 0.22614163160324097, 'KL Loss': 239.46875, 'Total Loss': 239.69488525390625}\n",
      "Epoch 35/600, Batch 1, Losses: {'Reconstruction Loss': 0.22190655767917633, 'KL Loss': 220.2738800048828, 'Total Loss': 220.49578857421875}\n",
      "Epoch 36/600, Batch 1, Losses: {'Reconstruction Loss': 0.239854633808136, 'KL Loss': 198.2953338623047, 'Total Loss': 198.53518676757812}\n",
      "Epoch 37/600, Batch 1, Losses: {'Reconstruction Loss': 0.1716460883617401, 'KL Loss': 170.914794921875, 'Total Loss': 171.08644104003906}\n",
      "Epoch 38/600, Batch 1, Losses: {'Reconstruction Loss': 0.1617700159549713, 'KL Loss': 133.2894287109375, 'Total Loss': 133.45120239257812}\n",
      "Epoch 39/600, Batch 1, Losses: {'Reconstruction Loss': 0.24857474863529205, 'KL Loss': 105.1502685546875, 'Total Loss': 105.39884185791016}\n",
      "Epoch 40/600, Batch 1, Losses: {'Reconstruction Loss': 0.2911030054092407, 'KL Loss': 106.24537658691406, 'Total Loss': 106.5364761352539}\n",
      "Epoch 41/600, Batch 1, Losses: {'Reconstruction Loss': 0.23773787915706635, 'KL Loss': 123.98973846435547, 'Total Loss': 124.22747802734375}\n",
      "Epoch 42/600, Batch 1, Losses: {'Reconstruction Loss': 0.19772300124168396, 'KL Loss': 123.78239440917969, 'Total Loss': 123.98011779785156}\n",
      "Epoch 43/600, Batch 1, Losses: {'Reconstruction Loss': 0.25813740491867065, 'KL Loss': 94.5916519165039, 'Total Loss': 94.84979248046875}\n",
      "Epoch 44/600, Batch 1, Losses: {'Reconstruction Loss': 0.29957231879234314, 'KL Loss': 63.24099349975586, 'Total Loss': 63.540565490722656}\n",
      "Epoch 45/600, Batch 1, Losses: {'Reconstruction Loss': 0.5078401565551758, 'KL Loss': 53.78809356689453, 'Total Loss': 54.29593276977539}\n",
      "Epoch 46/600, Batch 1, Losses: {'Reconstruction Loss': 0.6006034016609192, 'KL Loss': 63.24518585205078, 'Total Loss': 63.84579086303711}\n",
      "Epoch 47/600, Batch 1, Losses: {'Reconstruction Loss': 0.3756084144115448, 'KL Loss': 74.8599624633789, 'Total Loss': 75.2355728149414}\n",
      "Epoch 48/600, Batch 1, Losses: {'Reconstruction Loss': 0.2815382480621338, 'KL Loss': 73.89183044433594, 'Total Loss': 74.17337036132812}\n",
      "Epoch 49/600, Batch 1, Losses: {'Reconstruction Loss': 0.38446348905563354, 'KL Loss': 56.98551940917969, 'Total Loss': 57.3699836730957}\n",
      "Epoch 50/600, Batch 1, Losses: {'Reconstruction Loss': 0.4506106376647949, 'KL Loss': 36.9407844543457, 'Total Loss': 37.391395568847656}\n",
      "Epoch 51/600, Batch 1, Losses: {'Reconstruction Loss': 0.6111804246902466, 'KL Loss': 30.745031356811523, 'Total Loss': 31.356212615966797}\n",
      "Epoch 52/600, Batch 1, Losses: {'Reconstruction Loss': 0.6496163606643677, 'KL Loss': 38.36137771606445, 'Total Loss': 39.01099395751953}\n",
      "Epoch 53/600, Batch 1, Losses: {'Reconstruction Loss': 0.4026774764060974, 'KL Loss': 45.28559494018555, 'Total Loss': 45.688270568847656}\n",
      "Epoch 54/600, Batch 1, Losses: {'Reconstruction Loss': 0.3779488205909729, 'KL Loss': 42.456050872802734, 'Total Loss': 42.83399963378906}\n",
      "Epoch 55/600, Batch 1, Losses: {'Reconstruction Loss': 0.34938716888427734, 'KL Loss': 32.11648941040039, 'Total Loss': 32.465877532958984}\n",
      "Epoch 56/600, Batch 1, Losses: {'Reconstruction Loss': 0.25741562247276306, 'KL Loss': 21.481019973754883, 'Total Loss': 21.738435745239258}\n",
      "Epoch 57/600, Batch 1, Losses: {'Reconstruction Loss': 0.3012462854385376, 'KL Loss': 17.955320358276367, 'Total Loss': 18.256567001342773}\n",
      "Epoch 58/600, Batch 1, Losses: {'Reconstruction Loss': 0.37597060203552246, 'KL Loss': 22.035966873168945, 'Total Loss': 22.411937713623047}\n",
      "Epoch 59/600, Batch 1, Losses: {'Reconstruction Loss': 0.2715337574481964, 'KL Loss': 25.752426147460938, 'Total Loss': 26.02396011352539}\n",
      "Epoch 60/600, Batch 1, Losses: {'Reconstruction Loss': 0.17991584539413452, 'KL Loss': 23.129390716552734, 'Total Loss': 23.309307098388672}\n",
      "Epoch 61/600, Batch 1, Losses: {'Reconstruction Loss': 0.2225385308265686, 'KL Loss': 16.221242904663086, 'Total Loss': 16.44378089904785}\n",
      "Epoch 62/600, Batch 1, Losses: {'Reconstruction Loss': 0.24440881609916687, 'KL Loss': 11.109164237976074, 'Total Loss': 11.353572845458984}\n",
      "Epoch 63/600, Batch 1, Losses: {'Reconstruction Loss': 0.23979584872722626, 'KL Loss': 11.08718490600586, 'Total Loss': 11.326980590820312}\n",
      "Epoch 64/600, Batch 1, Losses: {'Reconstruction Loss': 0.21264049410820007, 'KL Loss': 13.538844108581543, 'Total Loss': 13.751484870910645}\n",
      "Epoch 65/600, Batch 1, Losses: {'Reconstruction Loss': 0.1657574623823166, 'KL Loss': 14.318072319030762, 'Total Loss': 14.483829498291016}\n",
      "Epoch 66/600, Batch 1, Losses: {'Reconstruction Loss': 0.1235867440700531, 'KL Loss': 11.985682487487793, 'Total Loss': 12.109269142150879}\n",
      "Epoch 67/600, Batch 1, Losses: {'Reconstruction Loss': 0.12441643327474594, 'KL Loss': 8.767236709594727, 'Total Loss': 8.891653060913086}\n",
      "Epoch 68/600, Batch 1, Losses: {'Reconstruction Loss': 0.15108652412891388, 'KL Loss': 6.761359214782715, 'Total Loss': 6.912445545196533}\n",
      "Epoch 69/600, Batch 1, Losses: {'Reconstruction Loss': 0.17644673585891724, 'KL Loss': 7.2368011474609375, 'Total Loss': 7.413248062133789}\n",
      "Epoch 70/600, Batch 1, Losses: {'Reconstruction Loss': 0.1714833378791809, 'KL Loss': 8.831474304199219, 'Total Loss': 9.002957344055176}\n",
      "Epoch 71/600, Batch 1, Losses: {'Reconstruction Loss': 0.14280597865581512, 'KL Loss': 8.998483657836914, 'Total Loss': 9.141289710998535}\n",
      "Epoch 72/600, Batch 1, Losses: {'Reconstruction Loss': 0.11645819991827011, 'KL Loss': 6.775273323059082, 'Total Loss': 6.8917317390441895}\n",
      "Epoch 73/600, Batch 1, Losses: {'Reconstruction Loss': 0.11721032112836838, 'KL Loss': 4.836764812469482, 'Total Loss': 4.953975200653076}\n",
      "Epoch 74/600, Batch 1, Losses: {'Reconstruction Loss': 0.1311390995979309, 'KL Loss': 4.916806697845459, 'Total Loss': 5.047945976257324}\n",
      "Epoch 75/600, Batch 1, Losses: {'Reconstruction Loss': 0.13678139448165894, 'KL Loss': 5.777090549468994, 'Total Loss': 5.913871765136719}\n",
      "Epoch 76/600, Batch 1, Losses: {'Reconstruction Loss': 0.12748216092586517, 'KL Loss': 6.134932994842529, 'Total Loss': 6.262414932250977}\n",
      "Epoch 77/600, Batch 1, Losses: {'Reconstruction Loss': 0.10194758325815201, 'KL Loss': 5.474197864532471, 'Total Loss': 5.576145648956299}\n",
      "Epoch 78/600, Batch 1, Losses: {'Reconstruction Loss': 0.0884929671883583, 'KL Loss': 4.027914047241211, 'Total Loss': 4.1164069175720215}\n",
      "Epoch 79/600, Batch 1, Losses: {'Reconstruction Loss': 0.09690950065851212, 'KL Loss': 3.478893518447876, 'Total Loss': 3.57580304145813}\n",
      "Epoch 80/600, Batch 1, Losses: {'Reconstruction Loss': 0.10692865401506424, 'KL Loss': 4.171223163604736, 'Total Loss': 4.278151988983154}\n",
      "Epoch 81/600, Batch 1, Losses: {'Reconstruction Loss': 0.10418123751878738, 'KL Loss': 4.603403091430664, 'Total Loss': 4.707584381103516}\n",
      "Epoch 82/600, Batch 1, Losses: {'Reconstruction Loss': 0.09352165460586548, 'KL Loss': 4.184750556945801, 'Total Loss': 4.2782721519470215}\n",
      "Epoch 83/600, Batch 1, Losses: {'Reconstruction Loss': 0.08772700279951096, 'KL Loss': 3.3390369415283203, 'Total Loss': 3.4267640113830566}\n",
      "Epoch 84/600, Batch 1, Losses: {'Reconstruction Loss': 0.08573894202709198, 'KL Loss': 2.8205034732818604, 'Total Loss': 2.9062423706054688}\n",
      "Epoch 85/600, Batch 1, Losses: {'Reconstruction Loss': 0.08144325762987137, 'KL Loss': 3.156085729598999, 'Total Loss': 3.2375290393829346}\n",
      "Epoch 86/600, Batch 1, Losses: {'Reconstruction Loss': 0.07650242745876312, 'KL Loss': 3.6008410453796387, 'Total Loss': 3.6773433685302734}\n",
      "Epoch 87/600, Batch 1, Losses: {'Reconstruction Loss': 0.07377280294895172, 'KL Loss': 3.317471981048584, 'Total Loss': 3.391244888305664}\n",
      "Epoch 88/600, Batch 1, Losses: {'Reconstruction Loss': 0.07520793378353119, 'KL Loss': 2.6745035648345947, 'Total Loss': 2.749711513519287}\n",
      "Epoch 89/600, Batch 1, Losses: {'Reconstruction Loss': 0.07808583229780197, 'KL Loss': 2.4108738899230957, 'Total Loss': 2.488959789276123}\n",
      "Epoch 90/600, Batch 1, Losses: {'Reconstruction Loss': 0.07775605469942093, 'KL Loss': 2.582054376602173, 'Total Loss': 2.6598105430603027}\n",
      "Epoch 91/600, Batch 1, Losses: {'Reconstruction Loss': 0.07335259765386581, 'KL Loss': 2.8187639713287354, 'Total Loss': 2.8921165466308594}\n",
      "Epoch 92/600, Batch 1, Losses: {'Reconstruction Loss': 0.06837957352399826, 'KL Loss': 2.606661081314087, 'Total Loss': 2.6750407218933105}\n",
      "Epoch 93/600, Batch 1, Losses: {'Reconstruction Loss': 0.06687179207801819, 'KL Loss': 2.23099946975708, 'Total Loss': 2.2978713512420654}\n",
      "Epoch 94/600, Batch 1, Losses: {'Reconstruction Loss': 0.06820470094680786, 'KL Loss': 2.1138482093811035, 'Total Loss': 2.1820528507232666}\n",
      "Epoch 95/600, Batch 1, Losses: {'Reconstruction Loss': 0.06869925558567047, 'KL Loss': 2.2477972507476807, 'Total Loss': 2.3164966106414795}\n",
      "Epoch 96/600, Batch 1, Losses: {'Reconstruction Loss': 0.06764832884073257, 'KL Loss': 2.3120431900024414, 'Total Loss': 2.3796916007995605}\n",
      "Epoch 97/600, Batch 1, Losses: {'Reconstruction Loss': 0.06394969671964645, 'KL Loss': 2.1522181034088135, 'Total Loss': 2.216167688369751}\n",
      "Epoch 98/600, Batch 1, Losses: {'Reconstruction Loss': 0.0613555908203125, 'KL Loss': 1.960213303565979, 'Total Loss': 2.021568775177002}\n",
      "Epoch 99/600, Batch 1, Losses: {'Reconstruction Loss': 0.061166178435087204, 'KL Loss': 1.8942803144454956, 'Total Loss': 1.955446481704712}\n",
      "Epoch 100/600, Batch 1, Losses: {'Reconstruction Loss': 0.0615219920873642, 'KL Loss': 1.9556422233581543, 'Total Loss': 2.0171642303466797}\n",
      "Epoch 101/600, Batch 1, Losses: {'Reconstruction Loss': 0.0625205710530281, 'KL Loss': 1.981874942779541, 'Total Loss': 2.0443954467773438}\n",
      "Epoch 102/600, Batch 1, Losses: {'Reconstruction Loss': 0.06177402287721634, 'KL Loss': 1.8787035942077637, 'Total Loss': 1.9404776096343994}\n",
      "Epoch 103/600, Batch 1, Losses: {'Reconstruction Loss': 0.059738025069236755, 'KL Loss': 1.7627754211425781, 'Total Loss': 1.822513461112976}\n",
      "Epoch 104/600, Batch 1, Losses: {'Reconstruction Loss': 0.05914369225502014, 'KL Loss': 1.7629204988479614, 'Total Loss': 1.8220641613006592}\n",
      "Epoch 105/600, Batch 1, Losses: {'Reconstruction Loss': 0.057973168790340424, 'KL Loss': 1.7465274333953857, 'Total Loss': 1.8045005798339844}\n",
      "Epoch 106/600, Batch 1, Losses: {'Reconstruction Loss': 0.05838803946971893, 'KL Loss': 1.772400140762329, 'Total Loss': 1.8307881355285645}\n",
      "Epoch 107/600, Batch 1, Losses: {'Reconstruction Loss': 0.05912649631500244, 'KL Loss': 1.698005199432373, 'Total Loss': 1.7571316957473755}\n",
      "Epoch 108/600, Batch 1, Losses: {'Reconstruction Loss': 0.05885438621044159, 'KL Loss': 1.622625708580017, 'Total Loss': 1.681480050086975}\n",
      "Epoch 109/600, Batch 1, Losses: {'Reconstruction Loss': 0.05912213400006294, 'KL Loss': 1.6186881065368652, 'Total Loss': 1.6778101921081543}\n",
      "Epoch 110/600, Batch 1, Losses: {'Reconstruction Loss': 0.05795422941446304, 'KL Loss': 1.6127586364746094, 'Total Loss': 1.6707128286361694}\n",
      "Epoch 111/600, Batch 1, Losses: {'Reconstruction Loss': 0.0569278821349144, 'KL Loss': 1.5993397235870361, 'Total Loss': 1.6562676429748535}\n",
      "Epoch 112/600, Batch 1, Losses: {'Reconstruction Loss': 0.05648212134838104, 'KL Loss': 1.5446360111236572, 'Total Loss': 1.6011180877685547}\n",
      "Epoch 113/600, Batch 1, Losses: {'Reconstruction Loss': 0.05661044642329216, 'KL Loss': 1.5098278522491455, 'Total Loss': 1.5664383172988892}\n",
      "Epoch 114/600, Batch 1, Losses: {'Reconstruction Loss': 0.056668251752853394, 'KL Loss': 1.4907342195510864, 'Total Loss': 1.5474025011062622}\n",
      "Epoch 115/600, Batch 1, Losses: {'Reconstruction Loss': 0.0563543327152729, 'KL Loss': 1.466495156288147, 'Total Loss': 1.522849440574646}\n",
      "Epoch 116/600, Batch 1, Losses: {'Reconstruction Loss': 0.056625887751579285, 'KL Loss': 1.4594175815582275, 'Total Loss': 1.5160434246063232}\n",
      "Epoch 117/600, Batch 1, Losses: {'Reconstruction Loss': 0.056294746696949005, 'KL Loss': 1.4360523223876953, 'Total Loss': 1.4923471212387085}\n",
      "Epoch 118/600, Batch 1, Losses: {'Reconstruction Loss': 0.05503176525235176, 'KL Loss': 1.3886890411376953, 'Total Loss': 1.443720817565918}\n",
      "Epoch 119/600, Batch 1, Losses: {'Reconstruction Loss': 0.05544617399573326, 'KL Loss': 1.3746792078018188, 'Total Loss': 1.43012535572052}\n",
      "Epoch 120/600, Batch 1, Losses: {'Reconstruction Loss': 0.05542523041367531, 'KL Loss': 1.3638571500778198, 'Total Loss': 1.4192824363708496}\n",
      "Epoch 121/600, Batch 1, Losses: {'Reconstruction Loss': 0.05538976937532425, 'KL Loss': 1.3388878107070923, 'Total Loss': 1.394277572631836}\n",
      "Epoch 122/600, Batch 1, Losses: {'Reconstruction Loss': 0.05586910992860794, 'KL Loss': 1.3295230865478516, 'Total Loss': 1.385392189025879}\n",
      "Epoch 123/600, Batch 1, Losses: {'Reconstruction Loss': 0.05529187619686127, 'KL Loss': 1.2990167140960693, 'Total Loss': 1.3543086051940918}\n",
      "Epoch 124/600, Batch 1, Losses: {'Reconstruction Loss': 0.05493566766381264, 'KL Loss': 1.293610692024231, 'Total Loss': 1.3485463857650757}\n",
      "Epoch 125/600, Batch 1, Losses: {'Reconstruction Loss': 0.05490168184041977, 'KL Loss': 1.2706362009048462, 'Total Loss': 1.325537919998169}\n",
      "Epoch 126/600, Batch 1, Losses: {'Reconstruction Loss': 0.05497139319777489, 'KL Loss': 1.2615363597869873, 'Total Loss': 1.3165076971054077}\n",
      "Epoch 127/600, Batch 1, Losses: {'Reconstruction Loss': 0.05516162887215614, 'KL Loss': 1.2410423755645752, 'Total Loss': 1.2962039709091187}\n",
      "Epoch 128/600, Batch 1, Losses: {'Reconstruction Loss': 0.054820913821458817, 'KL Loss': 1.2251590490341187, 'Total Loss': 1.279979944229126}\n",
      "Epoch 129/600, Batch 1, Losses: {'Reconstruction Loss': 0.05506893992424011, 'KL Loss': 1.206533432006836, 'Total Loss': 1.2616024017333984}\n",
      "Epoch 130/600, Batch 1, Losses: {'Reconstruction Loss': 0.05451617017388344, 'KL Loss': 1.19697105884552, 'Total Loss': 1.2514872550964355}\n",
      "Epoch 131/600, Batch 1, Losses: {'Reconstruction Loss': 0.05465470626950264, 'KL Loss': 1.1941813230514526, 'Total Loss': 1.2488360404968262}\n",
      "Epoch 132/600, Batch 1, Losses: {'Reconstruction Loss': 0.054717399179935455, 'KL Loss': 1.1554094552993774, 'Total Loss': 1.2101268768310547}\n",
      "Epoch 133/600, Batch 1, Losses: {'Reconstruction Loss': 0.05406605079770088, 'KL Loss': 1.149820327758789, 'Total Loss': 1.2038863897323608}\n",
      "Epoch 134/600, Batch 1, Losses: {'Reconstruction Loss': 0.05377986282110214, 'KL Loss': 1.1336486339569092, 'Total Loss': 1.1874284744262695}\n",
      "Epoch 135/600, Batch 1, Losses: {'Reconstruction Loss': 0.05400735139846802, 'KL Loss': 1.132357120513916, 'Total Loss': 1.1863644123077393}\n",
      "Epoch 136/600, Batch 1, Losses: {'Reconstruction Loss': 0.05390825495123863, 'KL Loss': 1.1207150220870972, 'Total Loss': 1.1746232509613037}\n",
      "Epoch 137/600, Batch 1, Losses: {'Reconstruction Loss': 0.05398481711745262, 'KL Loss': 1.0942538976669312, 'Total Loss': 1.1482386589050293}\n",
      "Epoch 138/600, Batch 1, Losses: {'Reconstruction Loss': 0.05421571061015129, 'KL Loss': 1.0872955322265625, 'Total Loss': 1.1415112018585205}\n",
      "Epoch 139/600, Batch 1, Losses: {'Reconstruction Loss': 0.054007500410079956, 'KL Loss': 1.0812844038009644, 'Total Loss': 1.1352919340133667}\n",
      "Epoch 140/600, Batch 1, Losses: {'Reconstruction Loss': 0.05380823090672493, 'KL Loss': 1.0703688859939575, 'Total Loss': 1.124177098274231}\n",
      "Epoch 141/600, Batch 1, Losses: {'Reconstruction Loss': 0.05358937010169029, 'KL Loss': 1.0548954010009766, 'Total Loss': 1.1084847450256348}\n",
      "Epoch 142/600, Batch 1, Losses: {'Reconstruction Loss': 0.05361084267497063, 'KL Loss': 1.0316720008850098, 'Total Loss': 1.085282802581787}\n",
      "Epoch 143/600, Batch 1, Losses: {'Reconstruction Loss': 0.05361589044332504, 'KL Loss': 1.0369709730148315, 'Total Loss': 1.0905869007110596}\n",
      "Epoch 144/600, Batch 1, Losses: {'Reconstruction Loss': 0.053466979414224625, 'KL Loss': 1.0175495147705078, 'Total Loss': 1.071016550064087}\n",
      "Epoch 145/600, Batch 1, Losses: {'Reconstruction Loss': 0.05418084189295769, 'KL Loss': 1.0172336101531982, 'Total Loss': 1.0714144706726074}\n",
      "Epoch 146/600, Batch 1, Losses: {'Reconstruction Loss': 0.05342715233564377, 'KL Loss': 0.9883829355239868, 'Total Loss': 1.0418100357055664}\n",
      "Epoch 147/600, Batch 1, Losses: {'Reconstruction Loss': 0.05313275381922722, 'KL Loss': 0.9804811477661133, 'Total Loss': 1.033613920211792}\n",
      "Epoch 148/600, Batch 1, Losses: {'Reconstruction Loss': 0.053379468619823456, 'KL Loss': 0.9790235161781311, 'Total Loss': 1.0324029922485352}\n",
      "Epoch 149/600, Batch 1, Losses: {'Reconstruction Loss': 0.05423719808459282, 'KL Loss': 0.9823862910270691, 'Total Loss': 1.036623477935791}\n",
      "Epoch 150/600, Batch 1, Losses: {'Reconstruction Loss': 0.054047539830207825, 'KL Loss': 0.9585232734680176, 'Total Loss': 1.012570858001709}\n",
      "Epoch 151/600, Batch 1, Losses: {'Reconstruction Loss': 0.054095298051834106, 'KL Loss': 0.9574084877967834, 'Total Loss': 1.01150381565094}\n",
      "Epoch 152/600, Batch 1, Losses: {'Reconstruction Loss': 0.05390122905373573, 'KL Loss': 0.9434878826141357, 'Total Loss': 0.9973891377449036}\n",
      "Epoch 153/600, Batch 1, Losses: {'Reconstruction Loss': 0.05294056609272957, 'KL Loss': 0.9195513129234314, 'Total Loss': 0.9724918603897095}\n",
      "Epoch 154/600, Batch 1, Losses: {'Reconstruction Loss': 0.05327155441045761, 'KL Loss': 0.9301851987838745, 'Total Loss': 0.9834567308425903}\n",
      "Epoch 155/600, Batch 1, Losses: {'Reconstruction Loss': 0.05344623327255249, 'KL Loss': 0.9208260774612427, 'Total Loss': 0.9742723107337952}\n",
      "Epoch 156/600, Batch 1, Losses: {'Reconstruction Loss': 0.05316226929426193, 'KL Loss': 0.8961864113807678, 'Total Loss': 0.9493486881256104}\n",
      "Epoch 157/600, Batch 1, Losses: {'Reconstruction Loss': 0.053313933312892914, 'KL Loss': 0.8949365019798279, 'Total Loss': 0.948250412940979}\n",
      "Epoch 158/600, Batch 1, Losses: {'Reconstruction Loss': 0.05300626531243324, 'KL Loss': 0.8866580724716187, 'Total Loss': 0.939664363861084}\n",
      "Epoch 159/600, Batch 1, Losses: {'Reconstruction Loss': 0.053451888263225555, 'KL Loss': 0.8832058310508728, 'Total Loss': 0.936657726764679}\n",
      "Epoch 160/600, Batch 1, Losses: {'Reconstruction Loss': 0.05290660634636879, 'KL Loss': 0.869417667388916, 'Total Loss': 0.9223242998123169}\n",
      "Epoch 161/600, Batch 1, Losses: {'Reconstruction Loss': 0.05312086641788483, 'KL Loss': 0.8600318431854248, 'Total Loss': 0.9131526947021484}\n",
      "Epoch 162/600, Batch 1, Losses: {'Reconstruction Loss': 0.052984222769737244, 'KL Loss': 0.8434346914291382, 'Total Loss': 0.8964189291000366}\n",
      "Epoch 163/600, Batch 1, Losses: {'Reconstruction Loss': 0.05317411571741104, 'KL Loss': 0.8561440706253052, 'Total Loss': 0.909318208694458}\n",
      "Epoch 164/600, Batch 1, Losses: {'Reconstruction Loss': 0.05318852514028549, 'KL Loss': 0.8410282731056213, 'Total Loss': 0.894216775894165}\n",
      "Epoch 165/600, Batch 1, Losses: {'Reconstruction Loss': 0.05312573164701462, 'KL Loss': 0.8328084945678711, 'Total Loss': 0.8859342336654663}\n",
      "Epoch 166/600, Batch 1, Losses: {'Reconstruction Loss': 0.05297878757119179, 'KL Loss': 0.8235251903533936, 'Total Loss': 0.8765040040016174}\n",
      "Epoch 167/600, Batch 1, Losses: {'Reconstruction Loss': 0.05297015607357025, 'KL Loss': 0.8008581399917603, 'Total Loss': 0.8538283109664917}\n",
      "Epoch 168/600, Batch 1, Losses: {'Reconstruction Loss': 0.053175896406173706, 'KL Loss': 0.806713879108429, 'Total Loss': 0.8598897457122803}\n",
      "Epoch 169/600, Batch 1, Losses: {'Reconstruction Loss': 0.053182173520326614, 'KL Loss': 0.7991965413093567, 'Total Loss': 0.8523787260055542}\n",
      "Epoch 170/600, Batch 1, Losses: {'Reconstruction Loss': 0.05320348963141441, 'KL Loss': 0.7988384962081909, 'Total Loss': 0.8520419597625732}\n",
      "Epoch 171/600, Batch 1, Losses: {'Reconstruction Loss': 0.053286559879779816, 'KL Loss': 0.788924515247345, 'Total Loss': 0.8422110676765442}\n",
      "Epoch 172/600, Batch 1, Losses: {'Reconstruction Loss': 0.052810944616794586, 'KL Loss': 0.772429883480072, 'Total Loss': 0.8252408504486084}\n",
      "Epoch 173/600, Batch 1, Losses: {'Reconstruction Loss': 0.05302360653877258, 'KL Loss': 0.7737981081008911, 'Total Loss': 0.8268216848373413}\n",
      "Epoch 174/600, Batch 1, Losses: {'Reconstruction Loss': 0.05299730226397514, 'KL Loss': 0.7753025889396667, 'Total Loss': 0.828299880027771}\n",
      "Epoch 175/600, Batch 1, Losses: {'Reconstruction Loss': 0.05305667966604233, 'KL Loss': 0.761033833026886, 'Total Loss': 0.8140904903411865}\n",
      "Epoch 176/600, Batch 1, Losses: {'Reconstruction Loss': 0.05288085341453552, 'KL Loss': 0.7484866976737976, 'Total Loss': 0.8013675212860107}\n",
      "Epoch 177/600, Batch 1, Losses: {'Reconstruction Loss': 0.05331481993198395, 'KL Loss': 0.7504870891571045, 'Total Loss': 0.8038018941879272}\n",
      "Epoch 178/600, Batch 1, Losses: {'Reconstruction Loss': 0.05268498510122299, 'KL Loss': 0.7319547533988953, 'Total Loss': 0.7846397161483765}\n",
      "Epoch 179/600, Batch 1, Losses: {'Reconstruction Loss': 0.05290243402123451, 'KL Loss': 0.7330774664878845, 'Total Loss': 0.7859799265861511}\n",
      "Epoch 180/600, Batch 1, Losses: {'Reconstruction Loss': 0.052515219897031784, 'KL Loss': 0.7250996232032776, 'Total Loss': 0.7776148319244385}\n",
      "Epoch 181/600, Batch 1, Losses: {'Reconstruction Loss': 0.05298367887735367, 'KL Loss': 0.7156025767326355, 'Total Loss': 0.768586277961731}\n",
      "Epoch 182/600, Batch 1, Losses: {'Reconstruction Loss': 0.052828095853328705, 'KL Loss': 0.7150648832321167, 'Total Loss': 0.7678929567337036}\n",
      "Epoch 183/600, Batch 1, Losses: {'Reconstruction Loss': 0.0533711314201355, 'KL Loss': 0.7172493934631348, 'Total Loss': 0.7706205248832703}\n",
      "Epoch 184/600, Batch 1, Losses: {'Reconstruction Loss': 0.05302887037396431, 'KL Loss': 0.7037844061851501, 'Total Loss': 0.7568132877349854}\n",
      "Epoch 185/600, Batch 1, Losses: {'Reconstruction Loss': 0.05293029546737671, 'KL Loss': 0.7001646757125854, 'Total Loss': 0.7530949711799622}\n",
      "Epoch 186/600, Batch 1, Losses: {'Reconstruction Loss': 0.05285827815532684, 'KL Loss': 0.6878082156181335, 'Total Loss': 0.7406665086746216}\n",
      "Epoch 187/600, Batch 1, Losses: {'Reconstruction Loss': 0.05271166190505028, 'KL Loss': 0.6818939447402954, 'Total Loss': 0.734605610370636}\n",
      "Epoch 188/600, Batch 1, Losses: {'Reconstruction Loss': 0.05288132280111313, 'KL Loss': 0.6795575618743896, 'Total Loss': 0.732438862323761}\n",
      "Epoch 189/600, Batch 1, Losses: {'Reconstruction Loss': 0.05290878191590309, 'KL Loss': 0.6783976554870605, 'Total Loss': 0.7313064336776733}\n",
      "Epoch 190/600, Batch 1, Losses: {'Reconstruction Loss': 0.052237365394830704, 'KL Loss': 0.6590683460235596, 'Total Loss': 0.7113057374954224}\n",
      "Epoch 191/600, Batch 1, Losses: {'Reconstruction Loss': 0.05231890827417374, 'KL Loss': 0.6549844741821289, 'Total Loss': 0.7073034048080444}\n",
      "Epoch 192/600, Batch 1, Losses: {'Reconstruction Loss': 0.053126730024814606, 'KL Loss': 0.6563147306442261, 'Total Loss': 0.7094414830207825}\n",
      "Epoch 193/600, Batch 1, Losses: {'Reconstruction Loss': 0.05304856598377228, 'KL Loss': 0.6528921127319336, 'Total Loss': 0.7059406638145447}\n",
      "Epoch 194/600, Batch 1, Losses: {'Reconstruction Loss': 0.052729930728673935, 'KL Loss': 0.6462453603744507, 'Total Loss': 0.6989752650260925}\n",
      "Epoch 195/600, Batch 1, Losses: {'Reconstruction Loss': 0.05239984020590782, 'KL Loss': 0.6425243020057678, 'Total Loss': 0.6949241161346436}\n",
      "Epoch 196/600, Batch 1, Losses: {'Reconstruction Loss': 0.052876587957143784, 'KL Loss': 0.628050684928894, 'Total Loss': 0.6809272766113281}\n",
      "Epoch 197/600, Batch 1, Losses: {'Reconstruction Loss': 0.05298323184251785, 'KL Loss': 0.6336329579353333, 'Total Loss': 0.6866161823272705}\n",
      "Epoch 198/600, Batch 1, Losses: {'Reconstruction Loss': 0.05265185981988907, 'KL Loss': 0.6224517822265625, 'Total Loss': 0.6751036643981934}\n",
      "Epoch 199/600, Batch 1, Losses: {'Reconstruction Loss': 0.05199790373444557, 'KL Loss': 0.6112384796142578, 'Total Loss': 0.6632363796234131}\n",
      "Epoch 200/600, Batch 1, Losses: {'Reconstruction Loss': 0.053014032542705536, 'KL Loss': 0.6182299852371216, 'Total Loss': 0.6712440252304077}\n",
      "Epoch 201/600, Batch 1, Losses: {'Reconstruction Loss': 0.052834607660770416, 'KL Loss': 0.6086927056312561, 'Total Loss': 0.6615273356437683}\n",
      "Epoch 202/600, Batch 1, Losses: {'Reconstruction Loss': 0.05275314673781395, 'KL Loss': 0.6079509258270264, 'Total Loss': 0.6607040762901306}\n",
      "Epoch 203/600, Batch 1, Losses: {'Reconstruction Loss': 0.052421361207962036, 'KL Loss': 0.6008725166320801, 'Total Loss': 0.6532938480377197}\n",
      "Epoch 204/600, Batch 1, Losses: {'Reconstruction Loss': 0.052719902247190475, 'KL Loss': 0.5979339480400085, 'Total Loss': 0.6506538391113281}\n",
      "Epoch 205/600, Batch 1, Losses: {'Reconstruction Loss': 0.052895475178956985, 'KL Loss': 0.5932319164276123, 'Total Loss': 0.6461274027824402}\n",
      "Epoch 206/600, Batch 1, Losses: {'Reconstruction Loss': 0.0527399405837059, 'KL Loss': 0.5834359526634216, 'Total Loss': 0.6361758708953857}\n",
      "Epoch 207/600, Batch 1, Losses: {'Reconstruction Loss': 0.05249088257551193, 'KL Loss': 0.5789839029312134, 'Total Loss': 0.6314747929573059}\n",
      "Epoch 208/600, Batch 1, Losses: {'Reconstruction Loss': 0.05252188444137573, 'KL Loss': 0.5727982521057129, 'Total Loss': 0.6253201365470886}\n",
      "Epoch 209/600, Batch 1, Losses: {'Reconstruction Loss': 0.052443649619817734, 'KL Loss': 0.5720977187156677, 'Total Loss': 0.6245413422584534}\n",
      "Epoch 210/600, Batch 1, Losses: {'Reconstruction Loss': 0.052377454936504364, 'KL Loss': 0.5663124322891235, 'Total Loss': 0.6186898946762085}\n",
      "Epoch 211/600, Batch 1, Losses: {'Reconstruction Loss': 0.052572593092918396, 'KL Loss': 0.5641120672225952, 'Total Loss': 0.6166846752166748}\n",
      "Epoch 212/600, Batch 1, Losses: {'Reconstruction Loss': 0.05236088111996651, 'KL Loss': 0.5543580055236816, 'Total Loss': 0.606718897819519}\n",
      "Epoch 213/600, Batch 1, Losses: {'Reconstruction Loss': 0.05242585018277168, 'KL Loss': 0.5487606525421143, 'Total Loss': 0.6011865139007568}\n",
      "Epoch 214/600, Batch 1, Losses: {'Reconstruction Loss': 0.05278719216585159, 'KL Loss': 0.5518237352371216, 'Total Loss': 0.6046109199523926}\n",
      "Epoch 215/600, Batch 1, Losses: {'Reconstruction Loss': 0.05218387767672539, 'KL Loss': 0.5382062792778015, 'Total Loss': 0.590390145778656}\n",
      "Epoch 216/600, Batch 1, Losses: {'Reconstruction Loss': 0.05203401669859886, 'KL Loss': 0.5382936596870422, 'Total Loss': 0.5903276801109314}\n",
      "Epoch 217/600, Batch 1, Losses: {'Reconstruction Loss': 0.052227284759283066, 'KL Loss': 0.531550943851471, 'Total Loss': 0.5837782025337219}\n",
      "Epoch 218/600, Batch 1, Losses: {'Reconstruction Loss': 0.05281568318605423, 'KL Loss': 0.5351424813270569, 'Total Loss': 0.5879581570625305}\n",
      "Epoch 219/600, Batch 1, Losses: {'Reconstruction Loss': 0.052291616797447205, 'KL Loss': 0.5266786217689514, 'Total Loss': 0.5789702534675598}\n",
      "Epoch 220/600, Batch 1, Losses: {'Reconstruction Loss': 0.052551474422216415, 'KL Loss': 0.5265397429466248, 'Total Loss': 0.5790911912918091}\n",
      "Epoch 221/600, Batch 1, Losses: {'Reconstruction Loss': 0.052401039749383926, 'KL Loss': 0.5229071974754333, 'Total Loss': 0.5753082633018494}\n",
      "Epoch 222/600, Batch 1, Losses: {'Reconstruction Loss': 0.052526798099279404, 'KL Loss': 0.5170259475708008, 'Total Loss': 0.5695527195930481}\n",
      "Epoch 223/600, Batch 1, Losses: {'Reconstruction Loss': 0.05258326232433319, 'KL Loss': 0.5161101818084717, 'Total Loss': 0.5686934590339661}\n",
      "Epoch 224/600, Batch 1, Losses: {'Reconstruction Loss': 0.05257998779416084, 'KL Loss': 0.509467601776123, 'Total Loss': 0.5620476007461548}\n",
      "Epoch 225/600, Batch 1, Losses: {'Reconstruction Loss': 0.052223872393369675, 'KL Loss': 0.5050090551376343, 'Total Loss': 0.5572329163551331}\n",
      "Epoch 226/600, Batch 1, Losses: {'Reconstruction Loss': 0.05231250077486038, 'KL Loss': 0.5060069561004639, 'Total Loss': 0.5583194494247437}\n",
      "Epoch 227/600, Batch 1, Losses: {'Reconstruction Loss': 0.05282696336507797, 'KL Loss': 0.5034889578819275, 'Total Loss': 0.5563158988952637}\n",
      "Epoch 228/600, Batch 1, Losses: {'Reconstruction Loss': 0.05217644199728966, 'KL Loss': 0.4955267906188965, 'Total Loss': 0.547703206539154}\n",
      "Epoch 229/600, Batch 1, Losses: {'Reconstruction Loss': 0.052667852491140366, 'KL Loss': 0.49290889501571655, 'Total Loss': 0.5455767512321472}\n",
      "Epoch 230/600, Batch 1, Losses: {'Reconstruction Loss': 0.0524667389690876, 'KL Loss': 0.4910077750682831, 'Total Loss': 0.5434744954109192}\n",
      "Epoch 231/600, Batch 1, Losses: {'Reconstruction Loss': 0.05241503193974495, 'KL Loss': 0.4832090139389038, 'Total Loss': 0.5356240272521973}\n",
      "Epoch 232/600, Batch 1, Losses: {'Reconstruction Loss': 0.05211413651704788, 'KL Loss': 0.48140355944633484, 'Total Loss': 0.5335177183151245}\n",
      "Epoch 233/600, Batch 1, Losses: {'Reconstruction Loss': 0.052312470972537994, 'KL Loss': 0.4754180312156677, 'Total Loss': 0.5277305245399475}\n",
      "Epoch 234/600, Batch 1, Losses: {'Reconstruction Loss': 0.05193854868412018, 'KL Loss': 0.4710570275783539, 'Total Loss': 0.5229955911636353}\n",
      "Epoch 235/600, Batch 1, Losses: {'Reconstruction Loss': 0.05253102257847786, 'KL Loss': 0.46902695298194885, 'Total Loss': 0.5215579867362976}\n",
      "Epoch 236/600, Batch 1, Losses: {'Reconstruction Loss': 0.052054498344659805, 'KL Loss': 0.4639396369457245, 'Total Loss': 0.515994131565094}\n",
      "Epoch 237/600, Batch 1, Losses: {'Reconstruction Loss': 0.052101943641901016, 'KL Loss': 0.4637640416622162, 'Total Loss': 0.5158659815788269}\n",
      "Epoch 238/600, Batch 1, Losses: {'Reconstruction Loss': 0.05271868407726288, 'KL Loss': 0.46142008900642395, 'Total Loss': 0.5141387581825256}\n",
      "Epoch 239/600, Batch 1, Losses: {'Reconstruction Loss': 0.052456147968769073, 'KL Loss': 0.45793378353118896, 'Total Loss': 0.5103899240493774}\n",
      "Epoch 240/600, Batch 1, Losses: {'Reconstruction Loss': 0.05252270773053169, 'KL Loss': 0.452953040599823, 'Total Loss': 0.5054757595062256}\n",
      "Epoch 241/600, Batch 1, Losses: {'Reconstruction Loss': 0.05189568176865578, 'KL Loss': 0.4457612931728363, 'Total Loss': 0.4976569712162018}\n",
      "Epoch 242/600, Batch 1, Losses: {'Reconstruction Loss': 0.05202769488096237, 'KL Loss': 0.44121477007865906, 'Total Loss': 0.493242472410202}\n",
      "Epoch 243/600, Batch 1, Losses: {'Reconstruction Loss': 0.052241384983062744, 'KL Loss': 0.43861842155456543, 'Total Loss': 0.4908598065376282}\n",
      "Epoch 244/600, Batch 1, Losses: {'Reconstruction Loss': 0.05215398222208023, 'KL Loss': 0.4460572600364685, 'Total Loss': 0.49821123480796814}\n",
      "Epoch 245/600, Batch 1, Losses: {'Reconstruction Loss': 0.05188152939081192, 'KL Loss': 0.4348096251487732, 'Total Loss': 0.4866911470890045}\n",
      "Epoch 246/600, Batch 1, Losses: {'Reconstruction Loss': 0.05206787586212158, 'KL Loss': 0.4347172677516937, 'Total Loss': 0.4867851436138153}\n",
      "Epoch 247/600, Batch 1, Losses: {'Reconstruction Loss': 0.05237109214067459, 'KL Loss': 0.4320583939552307, 'Total Loss': 0.4844294786453247}\n",
      "Epoch 248/600, Batch 1, Losses: {'Reconstruction Loss': 0.052450649440288544, 'KL Loss': 0.429073691368103, 'Total Loss': 0.48152434825897217}\n",
      "Epoch 249/600, Batch 1, Losses: {'Reconstruction Loss': 0.05226229131221771, 'KL Loss': 0.4208707809448242, 'Total Loss': 0.4731330871582031}\n",
      "Epoch 250/600, Batch 1, Losses: {'Reconstruction Loss': 0.0523453913629055, 'KL Loss': 0.42363250255584717, 'Total Loss': 0.47597789764404297}\n",
      "Epoch 251/600, Batch 1, Losses: {'Reconstruction Loss': 0.05231780931353569, 'KL Loss': 0.41780391335487366, 'Total Loss': 0.47012171149253845}\n",
      "Epoch 252/600, Batch 1, Losses: {'Reconstruction Loss': 0.05263247340917587, 'KL Loss': 0.41780325770378113, 'Total Loss': 0.4704357385635376}\n",
      "Epoch 253/600, Batch 1, Losses: {'Reconstruction Loss': 0.05266481637954712, 'KL Loss': 0.4176003932952881, 'Total Loss': 0.4702652096748352}\n",
      "Epoch 254/600, Batch 1, Losses: {'Reconstruction Loss': 0.051577333360910416, 'KL Loss': 0.40603119134902954, 'Total Loss': 0.45760852098464966}\n",
      "Epoch 255/600, Batch 1, Losses: {'Reconstruction Loss': 0.052348725497722626, 'KL Loss': 0.4069472551345825, 'Total Loss': 0.45929598808288574}\n",
      "Epoch 256/600, Batch 1, Losses: {'Reconstruction Loss': 0.052043356001377106, 'KL Loss': 0.40874379873275757, 'Total Loss': 0.4607871472835541}\n",
      "Epoch 257/600, Batch 1, Losses: {'Reconstruction Loss': 0.05208229646086693, 'KL Loss': 0.4029015004634857, 'Total Loss': 0.45498380064964294}\n",
      "Epoch 258/600, Batch 1, Losses: {'Reconstruction Loss': 0.05199824646115303, 'KL Loss': 0.40066877007484436, 'Total Loss': 0.4526670277118683}\n",
      "Epoch 259/600, Batch 1, Losses: {'Reconstruction Loss': 0.05196351930499077, 'KL Loss': 0.39492368698120117, 'Total Loss': 0.44688719511032104}\n",
      "Epoch 260/600, Batch 1, Losses: {'Reconstruction Loss': 0.05211829021573067, 'KL Loss': 0.39950019121170044, 'Total Loss': 0.451618492603302}\n",
      "Epoch 261/600, Batch 1, Losses: {'Reconstruction Loss': 0.05244235321879387, 'KL Loss': 0.3930959105491638, 'Total Loss': 0.4455382525920868}\n",
      "Epoch 262/600, Batch 1, Losses: {'Reconstruction Loss': 0.05214153230190277, 'KL Loss': 0.38388586044311523, 'Total Loss': 0.4360274076461792}\n",
      "Epoch 263/600, Batch 1, Losses: {'Reconstruction Loss': 0.05262644961476326, 'KL Loss': 0.39066532254219055, 'Total Loss': 0.4432917833328247}\n",
      "Epoch 264/600, Batch 1, Losses: {'Reconstruction Loss': 0.052080292254686356, 'KL Loss': 0.3818581700325012, 'Total Loss': 0.43393847346305847}\n",
      "Epoch 265/600, Batch 1, Losses: {'Reconstruction Loss': 0.05182049423456192, 'KL Loss': 0.3784330487251282, 'Total Loss': 0.4302535355091095}\n",
      "Epoch 266/600, Batch 1, Losses: {'Reconstruction Loss': 0.05161534249782562, 'KL Loss': 0.3765040338039398, 'Total Loss': 0.42811936140060425}\n",
      "Epoch 267/600, Batch 1, Losses: {'Reconstruction Loss': 0.052477989345788956, 'KL Loss': 0.38100358843803406, 'Total Loss': 0.4334815740585327}\n",
      "Epoch 268/600, Batch 1, Losses: {'Reconstruction Loss': 0.051828935742378235, 'KL Loss': 0.37320631742477417, 'Total Loss': 0.4250352382659912}\n",
      "Epoch 269/600, Batch 1, Losses: {'Reconstruction Loss': 0.05221250653266907, 'KL Loss': 0.3696015477180481, 'Total Loss': 0.42181405425071716}\n",
      "Epoch 270/600, Batch 1, Losses: {'Reconstruction Loss': 0.052686721086502075, 'KL Loss': 0.3751199543476105, 'Total Loss': 0.42780667543411255}\n",
      "Epoch 271/600, Batch 1, Losses: {'Reconstruction Loss': 0.0519716702401638, 'KL Loss': 0.36457595229148865, 'Total Loss': 0.41654762625694275}\n",
      "Epoch 272/600, Batch 1, Losses: {'Reconstruction Loss': 0.052012767642736435, 'KL Loss': 0.36579930782318115, 'Total Loss': 0.4178120791912079}\n",
      "Epoch 273/600, Batch 1, Losses: {'Reconstruction Loss': 0.052022092044353485, 'KL Loss': 0.36240848898887634, 'Total Loss': 0.4144305884838104}\n",
      "Epoch 274/600, Batch 1, Losses: {'Reconstruction Loss': 0.05214184895157814, 'KL Loss': 0.35464099049568176, 'Total Loss': 0.4067828357219696}\n",
      "Epoch 275/600, Batch 1, Losses: {'Reconstruction Loss': 0.051818229258060455, 'KL Loss': 0.3567923903465271, 'Total Loss': 0.40861061215400696}\n",
      "Epoch 276/600, Batch 1, Losses: {'Reconstruction Loss': 0.05200597643852234, 'KL Loss': 0.35405510663986206, 'Total Loss': 0.4060610830783844}\n",
      "Epoch 277/600, Batch 1, Losses: {'Reconstruction Loss': 0.05216265097260475, 'KL Loss': 0.35753488540649414, 'Total Loss': 0.4096975326538086}\n",
      "Epoch 278/600, Batch 1, Losses: {'Reconstruction Loss': 0.05179840698838234, 'KL Loss': 0.3511642515659332, 'Total Loss': 0.40296265482902527}\n",
      "Epoch 279/600, Batch 1, Losses: {'Reconstruction Loss': 0.052268631756305695, 'KL Loss': 0.35134002566337585, 'Total Loss': 0.40360864996910095}\n",
      "Epoch 280/600, Batch 1, Losses: {'Reconstruction Loss': 0.05226705223321915, 'KL Loss': 0.3467731177806854, 'Total Loss': 0.399040162563324}\n",
      "Epoch 281/600, Batch 1, Losses: {'Reconstruction Loss': 0.05176278203725815, 'KL Loss': 0.34328654408454895, 'Total Loss': 0.3950493335723877}\n",
      "Epoch 282/600, Batch 1, Losses: {'Reconstruction Loss': 0.05206577852368355, 'KL Loss': 0.3420836925506592, 'Total Loss': 0.3941494822502136}\n",
      "Epoch 283/600, Batch 1, Losses: {'Reconstruction Loss': 0.051981329917907715, 'KL Loss': 0.34202298521995544, 'Total Loss': 0.39400431513786316}\n",
      "Epoch 284/600, Batch 1, Losses: {'Reconstruction Loss': 0.05189226567745209, 'KL Loss': 0.33752575516700745, 'Total Loss': 0.38941800594329834}\n",
      "Epoch 285/600, Batch 1, Losses: {'Reconstruction Loss': 0.05219436064362526, 'KL Loss': 0.33639442920684814, 'Total Loss': 0.3885887861251831}\n",
      "Epoch 286/600, Batch 1, Losses: {'Reconstruction Loss': 0.05148177221417427, 'KL Loss': 0.3349608778953552, 'Total Loss': 0.3864426612854004}\n",
      "Epoch 287/600, Batch 1, Losses: {'Reconstruction Loss': 0.052282966673374176, 'KL Loss': 0.3295045793056488, 'Total Loss': 0.3817875385284424}\n",
      "Epoch 288/600, Batch 1, Losses: {'Reconstruction Loss': 0.05191320925951004, 'KL Loss': 0.3286982476711273, 'Total Loss': 0.38061144948005676}\n",
      "Epoch 289/600, Batch 1, Losses: {'Reconstruction Loss': 0.0520077720284462, 'KL Loss': 0.3268572688102722, 'Total Loss': 0.3788650333881378}\n",
      "Epoch 290/600, Batch 1, Losses: {'Reconstruction Loss': 0.051775749772787094, 'KL Loss': 0.32413941621780396, 'Total Loss': 0.37591516971588135}\n",
      "Epoch 291/600, Batch 1, Losses: {'Reconstruction Loss': 0.05212533846497536, 'KL Loss': 0.3237817883491516, 'Total Loss': 0.37590712308883667}\n",
      "Epoch 292/600, Batch 1, Losses: {'Reconstruction Loss': 0.05190557241439819, 'KL Loss': 0.3225385844707489, 'Total Loss': 0.3744441568851471}\n",
      "Epoch 293/600, Batch 1, Losses: {'Reconstruction Loss': 0.052426401525735855, 'KL Loss': 0.3216336965560913, 'Total Loss': 0.37406009435653687}\n",
      "Epoch 294/600, Batch 1, Losses: {'Reconstruction Loss': 0.05235792696475983, 'KL Loss': 0.3197450637817383, 'Total Loss': 0.3721029758453369}\n",
      "Epoch 295/600, Batch 1, Losses: {'Reconstruction Loss': 0.05196833238005638, 'KL Loss': 0.3189813792705536, 'Total Loss': 0.37094971537590027}\n",
      "Epoch 296/600, Batch 1, Losses: {'Reconstruction Loss': 0.05228961259126663, 'KL Loss': 0.3172871470451355, 'Total Loss': 0.36957675218582153}\n",
      "Epoch 297/600, Batch 1, Losses: {'Reconstruction Loss': 0.051922187209129333, 'KL Loss': 0.31278425455093384, 'Total Loss': 0.36470645666122437}\n",
      "Epoch 298/600, Batch 1, Losses: {'Reconstruction Loss': 0.0522281788289547, 'KL Loss': 0.31103524565696716, 'Total Loss': 0.36326342821121216}\n",
      "Epoch 299/600, Batch 1, Losses: {'Reconstruction Loss': 0.05208407714962959, 'KL Loss': 0.3080432713031769, 'Total Loss': 0.36012735962867737}\n",
      "Epoch 300/600, Batch 1, Losses: {'Reconstruction Loss': 0.05169825628399849, 'KL Loss': 0.302823543548584, 'Total Loss': 0.35452181100845337}\n",
      "Epoch 301/600, Batch 1, Losses: {'Reconstruction Loss': 0.052184924483299255, 'KL Loss': 0.3011850118637085, 'Total Loss': 0.35336995124816895}\n",
      "Epoch 302/600, Batch 1, Losses: {'Reconstruction Loss': 0.051542967557907104, 'KL Loss': 0.29759490489959717, 'Total Loss': 0.3491378724575043}\n",
      "Epoch 303/600, Batch 1, Losses: {'Reconstruction Loss': 0.0518549382686615, 'KL Loss': 0.3013206720352173, 'Total Loss': 0.3531756103038788}\n",
      "Epoch 304/600, Batch 1, Losses: {'Reconstruction Loss': 0.051518507301807404, 'KL Loss': 0.2952609956264496, 'Total Loss': 0.3467794954776764}\n",
      "Epoch 305/600, Batch 1, Losses: {'Reconstruction Loss': 0.05159831792116165, 'KL Loss': 0.2953050136566162, 'Total Loss': 0.34690332412719727}\n",
      "Epoch 306/600, Batch 1, Losses: {'Reconstruction Loss': 0.05153267830610275, 'KL Loss': 0.29217270016670227, 'Total Loss': 0.3437053859233856}\n",
      "Epoch 307/600, Batch 1, Losses: {'Reconstruction Loss': 0.051740601658821106, 'KL Loss': 0.29037177562713623, 'Total Loss': 0.34211236238479614}\n",
      "Epoch 308/600, Batch 1, Losses: {'Reconstruction Loss': 0.05192165449261665, 'KL Loss': 0.2922566533088684, 'Total Loss': 0.34417831897735596}\n",
      "Epoch 309/600, Batch 1, Losses: {'Reconstruction Loss': 0.05199515074491501, 'KL Loss': 0.291168212890625, 'Total Loss': 0.3431633710861206}\n",
      "Epoch 310/600, Batch 1, Losses: {'Reconstruction Loss': 0.05180380865931511, 'KL Loss': 0.2837259769439697, 'Total Loss': 0.33552977442741394}\n",
      "Epoch 311/600, Batch 1, Losses: {'Reconstruction Loss': 0.05140652507543564, 'KL Loss': 0.2857521176338196, 'Total Loss': 0.3371586501598358}\n",
      "Epoch 312/600, Batch 1, Losses: {'Reconstruction Loss': 0.0517011322081089, 'KL Loss': 0.281062513589859, 'Total Loss': 0.3327636420726776}\n",
      "Epoch 313/600, Batch 1, Losses: {'Reconstruction Loss': 0.05226845666766167, 'KL Loss': 0.28444886207580566, 'Total Loss': 0.33671730756759644}\n",
      "Epoch 314/600, Batch 1, Losses: {'Reconstruction Loss': 0.05184803158044815, 'KL Loss': 0.28158038854599, 'Total Loss': 0.33342841267585754}\n",
      "Epoch 315/600, Batch 1, Losses: {'Reconstruction Loss': 0.051815345883369446, 'KL Loss': 0.2769314646720886, 'Total Loss': 0.3287467956542969}\n",
      "Epoch 316/600, Batch 1, Losses: {'Reconstruction Loss': 0.051754243671894073, 'KL Loss': 0.27993345260620117, 'Total Loss': 0.33168768882751465}\n",
      "Epoch 317/600, Batch 1, Losses: {'Reconstruction Loss': 0.05184982344508171, 'KL Loss': 0.27897337079048157, 'Total Loss': 0.3308231830596924}\n",
      "Epoch 318/600, Batch 1, Losses: {'Reconstruction Loss': 0.052165139466524124, 'KL Loss': 0.2756351828575134, 'Total Loss': 0.32780033349990845}\n",
      "Epoch 319/600, Batch 1, Losses: {'Reconstruction Loss': 0.05225029215216637, 'KL Loss': 0.2730376720428467, 'Total Loss': 0.32528796792030334}\n",
      "Epoch 320/600, Batch 1, Losses: {'Reconstruction Loss': 0.051580075174570084, 'KL Loss': 0.27060404419898987, 'Total Loss': 0.32218411564826965}\n",
      "Epoch 321/600, Batch 1, Losses: {'Reconstruction Loss': 0.05207015946507454, 'KL Loss': 0.2682170271873474, 'Total Loss': 0.32028719782829285}\n",
      "Epoch 322/600, Batch 1, Losses: {'Reconstruction Loss': 0.05160517618060112, 'KL Loss': 0.26721706986427307, 'Total Loss': 0.3188222348690033}\n",
      "Epoch 323/600, Batch 1, Losses: {'Reconstruction Loss': 0.05203639343380928, 'KL Loss': 0.26759615540504456, 'Total Loss': 0.31963256001472473}\n",
      "Epoch 324/600, Batch 1, Losses: {'Reconstruction Loss': 0.05188717320561409, 'KL Loss': 0.26603373885154724, 'Total Loss': 0.3179209232330322}\n",
      "Epoch 325/600, Batch 1, Losses: {'Reconstruction Loss': 0.05163715034723282, 'KL Loss': 0.2626507878303528, 'Total Loss': 0.314287930727005}\n",
      "Epoch 326/600, Batch 1, Losses: {'Reconstruction Loss': 0.05182212218642235, 'KL Loss': 0.2623817026615143, 'Total Loss': 0.31420382857322693}\n",
      "Epoch 327/600, Batch 1, Losses: {'Reconstruction Loss': 0.051472678780555725, 'KL Loss': 0.25814732909202576, 'Total Loss': 0.3096200227737427}\n",
      "Epoch 328/600, Batch 1, Losses: {'Reconstruction Loss': 0.05250904709100723, 'KL Loss': 0.2613596022129059, 'Total Loss': 0.3138686418533325}\n",
      "Epoch 329/600, Batch 1, Losses: {'Reconstruction Loss': 0.05213722586631775, 'KL Loss': 0.26033756136894226, 'Total Loss': 0.31247478723526}\n",
      "Epoch 330/600, Batch 1, Losses: {'Reconstruction Loss': 0.05177723988890648, 'KL Loss': 0.25704410672187805, 'Total Loss': 0.30882135033607483}\n",
      "Epoch 331/600, Batch 1, Losses: {'Reconstruction Loss': 0.051624491810798645, 'KL Loss': 0.25429612398147583, 'Total Loss': 0.3059206008911133}\n",
      "Epoch 332/600, Batch 1, Losses: {'Reconstruction Loss': 0.05211792141199112, 'KL Loss': 0.2507835328578949, 'Total Loss': 0.3029014468193054}\n",
      "Epoch 333/600, Batch 1, Losses: {'Reconstruction Loss': 0.05224321782588959, 'KL Loss': 0.2485266774892807, 'Total Loss': 0.3007698953151703}\n",
      "Epoch 334/600, Batch 1, Losses: {'Reconstruction Loss': 0.051738057285547256, 'KL Loss': 0.2500762939453125, 'Total Loss': 0.30181434750556946}\n",
      "Epoch 335/600, Batch 1, Losses: {'Reconstruction Loss': 0.05206022411584854, 'KL Loss': 0.24810409545898438, 'Total Loss': 0.3001643121242523}\n",
      "Epoch 336/600, Batch 1, Losses: {'Reconstruction Loss': 0.05186805501580238, 'KL Loss': 0.24799275398254395, 'Total Loss': 0.29986080527305603}\n",
      "Epoch 337/600, Batch 1, Losses: {'Reconstruction Loss': 0.05182701349258423, 'KL Loss': 0.24664798378944397, 'Total Loss': 0.2984749972820282}\n",
      "Epoch 338/600, Batch 1, Losses: {'Reconstruction Loss': 0.051535047590732574, 'KL Loss': 0.2408476173877716, 'Total Loss': 0.2923826575279236}\n",
      "Epoch 339/600, Batch 1, Losses: {'Reconstruction Loss': 0.05148430913686752, 'KL Loss': 0.2420700490474701, 'Total Loss': 0.2935543656349182}\n",
      "Epoch 340/600, Batch 1, Losses: {'Reconstruction Loss': 0.051479797810316086, 'KL Loss': 0.2425694316625595, 'Total Loss': 0.2940492331981659}\n",
      "Epoch 341/600, Batch 1, Losses: {'Reconstruction Loss': 0.05223352089524269, 'KL Loss': 0.24242964386940002, 'Total Loss': 0.2946631610393524}\n",
      "Epoch 342/600, Batch 1, Losses: {'Reconstruction Loss': 0.05176956206560135, 'KL Loss': 0.23824328184127808, 'Total Loss': 0.29001283645629883}\n",
      "Epoch 343/600, Batch 1, Losses: {'Reconstruction Loss': 0.051722101867198944, 'KL Loss': 0.23654738068580627, 'Total Loss': 0.2882694900035858}\n",
      "Epoch 344/600, Batch 1, Losses: {'Reconstruction Loss': 0.05177579075098038, 'KL Loss': 0.23731574416160583, 'Total Loss': 0.2890915274620056}\n",
      "Epoch 345/600, Batch 1, Losses: {'Reconstruction Loss': 0.05190563201904297, 'KL Loss': 0.23653657734394073, 'Total Loss': 0.2884421944618225}\n",
      "Epoch 346/600, Batch 1, Losses: {'Reconstruction Loss': 0.0512944757938385, 'KL Loss': 0.22969168424606323, 'Total Loss': 0.28098616003990173}\n",
      "Epoch 347/600, Batch 1, Losses: {'Reconstruction Loss': 0.051190849393606186, 'KL Loss': 0.23120562732219696, 'Total Loss': 0.28239646553993225}\n",
      "Epoch 348/600, Batch 1, Losses: {'Reconstruction Loss': 0.05189388245344162, 'KL Loss': 0.2307400405406952, 'Total Loss': 0.2826339304447174}\n",
      "Epoch 349/600, Batch 1, Losses: {'Reconstruction Loss': 0.05153074860572815, 'KL Loss': 0.22771793603897095, 'Total Loss': 0.2792486846446991}\n",
      "Epoch 350/600, Batch 1, Losses: {'Reconstruction Loss': 0.051787957549095154, 'KL Loss': 0.22726111114025116, 'Total Loss': 0.2790490686893463}\n",
      "Epoch 351/600, Batch 1, Losses: {'Reconstruction Loss': 0.05177031084895134, 'KL Loss': 0.22552181780338287, 'Total Loss': 0.2772921323776245}\n",
      "Epoch 352/600, Batch 1, Losses: {'Reconstruction Loss': 0.051680564880371094, 'KL Loss': 0.2253342866897583, 'Total Loss': 0.2770148515701294}\n",
      "Epoch 353/600, Batch 1, Losses: {'Reconstruction Loss': 0.051533110439777374, 'KL Loss': 0.22505563497543335, 'Total Loss': 0.2765887379646301}\n",
      "Epoch 354/600, Batch 1, Losses: {'Reconstruction Loss': 0.05173031613230705, 'KL Loss': 0.22465689480304718, 'Total Loss': 0.27638721466064453}\n",
      "Epoch 355/600, Batch 1, Losses: {'Reconstruction Loss': 0.05175779387354851, 'KL Loss': 0.22318419814109802, 'Total Loss': 0.27494198083877563}\n",
      "Epoch 356/600, Batch 1, Losses: {'Reconstruction Loss': 0.05163382738828659, 'KL Loss': 0.22089707851409912, 'Total Loss': 0.2725309133529663}\n",
      "Epoch 357/600, Batch 1, Losses: {'Reconstruction Loss': 0.05215279012918472, 'KL Loss': 0.2209610790014267, 'Total Loss': 0.273113876581192}\n",
      "Epoch 358/600, Batch 1, Losses: {'Reconstruction Loss': 0.05171544477343559, 'KL Loss': 0.2205163538455963, 'Total Loss': 0.272231787443161}\n",
      "Epoch 359/600, Batch 1, Losses: {'Reconstruction Loss': 0.05209916830062866, 'KL Loss': 0.21995891630649567, 'Total Loss': 0.27205806970596313}\n",
      "Epoch 360/600, Batch 1, Losses: {'Reconstruction Loss': 0.051577046513557434, 'KL Loss': 0.2121281772851944, 'Total Loss': 0.26370522379875183}\n",
      "Epoch 361/600, Batch 1, Losses: {'Reconstruction Loss': 0.051568739116191864, 'KL Loss': 0.21306808292865753, 'Total Loss': 0.2646368145942688}\n",
      "Epoch 362/600, Batch 1, Losses: {'Reconstruction Loss': 0.05186406150460243, 'KL Loss': 0.21139609813690186, 'Total Loss': 0.263260155916214}\n",
      "Epoch 363/600, Batch 1, Losses: {'Reconstruction Loss': 0.05162665620446205, 'KL Loss': 0.20997865498065948, 'Total Loss': 0.26160532236099243}\n",
      "Epoch 364/600, Batch 1, Losses: {'Reconstruction Loss': 0.05169733613729477, 'KL Loss': 0.2105780392885208, 'Total Loss': 0.262275367975235}\n",
      "Epoch 365/600, Batch 1, Losses: {'Reconstruction Loss': 0.0517706461250782, 'KL Loss': 0.21093271672725677, 'Total Loss': 0.2627033591270447}\n",
      "Epoch 366/600, Batch 1, Losses: {'Reconstruction Loss': 0.05167112499475479, 'KL Loss': 0.20806677639484406, 'Total Loss': 0.25973790884017944}\n",
      "Epoch 367/600, Batch 1, Losses: {'Reconstruction Loss': 0.051600221544504166, 'KL Loss': 0.2078130841255188, 'Total Loss': 0.25941330194473267}\n",
      "Epoch 368/600, Batch 1, Losses: {'Reconstruction Loss': 0.05143299326300621, 'KL Loss': 0.205216646194458, 'Total Loss': 0.2566496431827545}\n",
      "Epoch 369/600, Batch 1, Losses: {'Reconstruction Loss': 0.05169439688324928, 'KL Loss': 0.20483291149139404, 'Total Loss': 0.256527304649353}\n",
      "Epoch 370/600, Batch 1, Losses: {'Reconstruction Loss': 0.051696471869945526, 'KL Loss': 0.20437243580818176, 'Total Loss': 0.2560689151287079}\n",
      "Epoch 371/600, Batch 1, Losses: {'Reconstruction Loss': 0.05185110494494438, 'KL Loss': 0.20444250106811523, 'Total Loss': 0.2562935948371887}\n",
      "Epoch 372/600, Batch 1, Losses: {'Reconstruction Loss': 0.051758453249931335, 'KL Loss': 0.20068815350532532, 'Total Loss': 0.25244659185409546}\n",
      "Epoch 373/600, Batch 1, Losses: {'Reconstruction Loss': 0.051529936492443085, 'KL Loss': 0.1999426931142807, 'Total Loss': 0.2514726221561432}\n",
      "Epoch 374/600, Batch 1, Losses: {'Reconstruction Loss': 0.05169551819562912, 'KL Loss': 0.20053134858608246, 'Total Loss': 0.252226859331131}\n",
      "Epoch 375/600, Batch 1, Losses: {'Reconstruction Loss': 0.051305536180734634, 'KL Loss': 0.19716818630695343, 'Total Loss': 0.24847371876239777}\n",
      "Epoch 376/600, Batch 1, Losses: {'Reconstruction Loss': 0.051887765526771545, 'KL Loss': 0.1975305676460266, 'Total Loss': 0.24941833317279816}\n",
      "Epoch 377/600, Batch 1, Losses: {'Reconstruction Loss': 0.051915690302848816, 'KL Loss': 0.19671152532100677, 'Total Loss': 0.2486272156238556}\n",
      "Epoch 378/600, Batch 1, Losses: {'Reconstruction Loss': 0.05185869708657265, 'KL Loss': 0.19635269045829773, 'Total Loss': 0.24821138381958008}\n",
      "Epoch 379/600, Batch 1, Losses: {'Reconstruction Loss': 0.05161180719733238, 'KL Loss': 0.19173623621463776, 'Total Loss': 0.24334804713726044}\n",
      "Epoch 380/600, Batch 1, Losses: {'Reconstruction Loss': 0.05161825194954872, 'KL Loss': 0.1948888748884201, 'Total Loss': 0.24650712311267853}\n",
      "Epoch 381/600, Batch 1, Losses: {'Reconstruction Loss': 0.05195418372750282, 'KL Loss': 0.19775553047657013, 'Total Loss': 0.24970971047878265}\n",
      "Epoch 382/600, Batch 1, Losses: {'Reconstruction Loss': 0.0516795814037323, 'KL Loss': 0.19260041415691376, 'Total Loss': 0.24427999556064606}\n",
      "Epoch 383/600, Batch 1, Losses: {'Reconstruction Loss': 0.05171986669301987, 'KL Loss': 0.1904052346944809, 'Total Loss': 0.24212509393692017}\n",
      "Epoch 384/600, Batch 1, Losses: {'Reconstruction Loss': 0.05218181759119034, 'KL Loss': 0.19076396524906158, 'Total Loss': 0.24294579029083252}\n",
      "Epoch 385/600, Batch 1, Losses: {'Reconstruction Loss': 0.05198865756392479, 'KL Loss': 0.18921169638633728, 'Total Loss': 0.24120035767555237}\n",
      "Epoch 386/600, Batch 1, Losses: {'Reconstruction Loss': 0.05194782465696335, 'KL Loss': 0.1898621767759323, 'Total Loss': 0.24180999398231506}\n",
      "Epoch 387/600, Batch 1, Losses: {'Reconstruction Loss': 0.051493965089321136, 'KL Loss': 0.1872902512550354, 'Total Loss': 0.23878422379493713}\n",
      "Epoch 388/600, Batch 1, Losses: {'Reconstruction Loss': 0.05166001245379448, 'KL Loss': 0.18514522910118103, 'Total Loss': 0.2368052452802658}\n",
      "Epoch 389/600, Batch 1, Losses: {'Reconstruction Loss': 0.0519217923283577, 'KL Loss': 0.18590153753757477, 'Total Loss': 0.23782333731651306}\n",
      "Epoch 390/600, Batch 1, Losses: {'Reconstruction Loss': 0.05178386718034744, 'KL Loss': 0.1826106160879135, 'Total Loss': 0.23439449071884155}\n",
      "Epoch 391/600, Batch 1, Losses: {'Reconstruction Loss': 0.05176465958356857, 'KL Loss': 0.18399633467197418, 'Total Loss': 0.23576098680496216}\n",
      "Epoch 392/600, Batch 1, Losses: {'Reconstruction Loss': 0.05152025818824768, 'KL Loss': 0.1844216287136078, 'Total Loss': 0.23594188690185547}\n",
      "Epoch 393/600, Batch 1, Losses: {'Reconstruction Loss': 0.0519251748919487, 'KL Loss': 0.18067999184131622, 'Total Loss': 0.23260515928268433}\n",
      "Epoch 394/600, Batch 1, Losses: {'Reconstruction Loss': 0.05200684443116188, 'KL Loss': 0.1820562481880188, 'Total Loss': 0.23406308889389038}\n",
      "Epoch 395/600, Batch 1, Losses: {'Reconstruction Loss': 0.05127432942390442, 'KL Loss': 0.1785680204629898, 'Total Loss': 0.22984234988689423}\n",
      "Epoch 396/600, Batch 1, Losses: {'Reconstruction Loss': 0.05188547447323799, 'KL Loss': 0.17864571511745453, 'Total Loss': 0.23053118586540222}\n",
      "Epoch 397/600, Batch 1, Losses: {'Reconstruction Loss': 0.05148402228951454, 'KL Loss': 0.17672091722488403, 'Total Loss': 0.22820493578910828}\n",
      "Epoch 398/600, Batch 1, Losses: {'Reconstruction Loss': 0.05189790949225426, 'KL Loss': 0.1764782965183258, 'Total Loss': 0.22837620973587036}\n",
      "Epoch 399/600, Batch 1, Losses: {'Reconstruction Loss': 0.051954224705696106, 'KL Loss': 0.1767858862876892, 'Total Loss': 0.22874011099338531}\n",
      "Epoch 400/600, Batch 1, Losses: {'Reconstruction Loss': 0.051622193306684494, 'KL Loss': 0.17581599950790405, 'Total Loss': 0.22743819653987885}\n",
      "Epoch 401/600, Batch 1, Losses: {'Reconstruction Loss': 0.051399219781160355, 'KL Loss': 0.174390509724617, 'Total Loss': 0.22578972578048706}\n",
      "Epoch 402/600, Batch 1, Losses: {'Reconstruction Loss': 0.05190831050276756, 'KL Loss': 0.17421375215053558, 'Total Loss': 0.22612206637859344}\n",
      "Epoch 403/600, Batch 1, Losses: {'Reconstruction Loss': 0.051801733672618866, 'KL Loss': 0.17190180718898773, 'Total Loss': 0.223703533411026}\n",
      "Epoch 404/600, Batch 1, Losses: {'Reconstruction Loss': 0.05188921466469765, 'KL Loss': 0.17128702998161316, 'Total Loss': 0.2231762409210205}\n",
      "Epoch 405/600, Batch 1, Losses: {'Reconstruction Loss': 0.051550183445215225, 'KL Loss': 0.16944529116153717, 'Total Loss': 0.2209954708814621}\n",
      "Epoch 406/600, Batch 1, Losses: {'Reconstruction Loss': 0.05163811892271042, 'KL Loss': 0.16855750977993011, 'Total Loss': 0.22019562125205994}\n",
      "Epoch 407/600, Batch 1, Losses: {'Reconstruction Loss': 0.05158863589167595, 'KL Loss': 0.16664470732212067, 'Total Loss': 0.21823334693908691}\n",
      "Epoch 408/600, Batch 1, Losses: {'Reconstruction Loss': 0.051472257822752, 'KL Loss': 0.1673017144203186, 'Total Loss': 0.2187739759683609}\n",
      "Epoch 409/600, Batch 1, Losses: {'Reconstruction Loss': 0.051644694060087204, 'KL Loss': 0.1645253747701645, 'Total Loss': 0.216170072555542}\n",
      "Epoch 410/600, Batch 1, Losses: {'Reconstruction Loss': 0.051741115748882294, 'KL Loss': 0.1666719615459442, 'Total Loss': 0.2184130847454071}\n",
      "Epoch 411/600, Batch 1, Losses: {'Reconstruction Loss': 0.05146436393260956, 'KL Loss': 0.16573624312877655, 'Total Loss': 0.2172006070613861}\n",
      "Epoch 412/600, Batch 1, Losses: {'Reconstruction Loss': 0.051844947040081024, 'KL Loss': 0.16374820470809937, 'Total Loss': 0.215593159198761}\n",
      "Epoch 413/600, Batch 1, Losses: {'Reconstruction Loss': 0.051294922828674316, 'KL Loss': 0.16237278282642365, 'Total Loss': 0.21366770565509796}\n",
      "Epoch 414/600, Batch 1, Losses: {'Reconstruction Loss': 0.05220465734601021, 'KL Loss': 0.16622979938983917, 'Total Loss': 0.21843445301055908}\n",
      "Epoch 415/600, Batch 1, Losses: {'Reconstruction Loss': 0.051849886775016785, 'KL Loss': 0.1645178198814392, 'Total Loss': 0.216367706656456}\n",
      "Epoch 416/600, Batch 1, Losses: {'Reconstruction Loss': 0.05180957168340683, 'KL Loss': 0.16354791820049286, 'Total Loss': 0.2153574824333191}\n",
      "Epoch 417/600, Batch 1, Losses: {'Reconstruction Loss': 0.05155918002128601, 'KL Loss': 0.15972910821437836, 'Total Loss': 0.21128828823566437}\n",
      "Epoch 418/600, Batch 1, Losses: {'Reconstruction Loss': 0.05191650614142418, 'KL Loss': 0.1615428775548935, 'Total Loss': 0.21345938742160797}\n",
      "Epoch 419/600, Batch 1, Losses: {'Reconstruction Loss': 0.051631368696689606, 'KL Loss': 0.15969295799732208, 'Total Loss': 0.21132433414459229}\n",
      "Epoch 420/600, Batch 1, Losses: {'Reconstruction Loss': 0.05163348838686943, 'KL Loss': 0.15659157931804657, 'Total Loss': 0.2082250714302063}\n",
      "Epoch 421/600, Batch 1, Losses: {'Reconstruction Loss': 0.05151204392313957, 'KL Loss': 0.157454714179039, 'Total Loss': 0.20896676182746887}\n",
      "Epoch 422/600, Batch 1, Losses: {'Reconstruction Loss': 0.051569096744060516, 'KL Loss': 0.15635137259960175, 'Total Loss': 0.20792046189308167}\n",
      "Epoch 423/600, Batch 1, Losses: {'Reconstruction Loss': 0.05170580372214317, 'KL Loss': 0.15536370873451233, 'Total Loss': 0.2070695161819458}\n",
      "Epoch 424/600, Batch 1, Losses: {'Reconstruction Loss': 0.05218983441591263, 'KL Loss': 0.15610167384147644, 'Total Loss': 0.20829150080680847}\n",
      "Epoch 425/600, Batch 1, Losses: {'Reconstruction Loss': 0.051733292639255524, 'KL Loss': 0.15573802590370178, 'Total Loss': 0.2074713110923767}\n",
      "Epoch 426/600, Batch 1, Losses: {'Reconstruction Loss': 0.051785241812467575, 'KL Loss': 0.15466512739658356, 'Total Loss': 0.20645037293434143}\n",
      "Epoch 427/600, Batch 1, Losses: {'Reconstruction Loss': 0.05184269696474075, 'KL Loss': 0.1526252031326294, 'Total Loss': 0.20446789264678955}\n",
      "Epoch 428/600, Batch 1, Losses: {'Reconstruction Loss': 0.05166688561439514, 'KL Loss': 0.1518825888633728, 'Total Loss': 0.20354947447776794}\n",
      "Epoch 429/600, Batch 1, Losses: {'Reconstruction Loss': 0.05194160342216492, 'KL Loss': 0.1533123254776001, 'Total Loss': 0.20525392889976501}\n",
      "Epoch 430/600, Batch 1, Losses: {'Reconstruction Loss': 0.051774609833955765, 'KL Loss': 0.1513773649930954, 'Total Loss': 0.20315197110176086}\n",
      "Epoch 431/600, Batch 1, Losses: {'Reconstruction Loss': 0.051780715584754944, 'KL Loss': 0.1497785598039627, 'Total Loss': 0.20155927538871765}\n",
      "Epoch 432/600, Batch 1, Losses: {'Reconstruction Loss': 0.05133472755551338, 'KL Loss': 0.1494763195514679, 'Total Loss': 0.20081104338169098}\n",
      "Epoch 433/600, Batch 1, Losses: {'Reconstruction Loss': 0.051999643445014954, 'KL Loss': 0.1495601385831833, 'Total Loss': 0.20155978202819824}\n",
      "Epoch 434/600, Batch 1, Losses: {'Reconstruction Loss': 0.0513543039560318, 'KL Loss': 0.14764006435871124, 'Total Loss': 0.19899436831474304}\n",
      "Epoch 435/600, Batch 1, Losses: {'Reconstruction Loss': 0.051444150507450104, 'KL Loss': 0.14676563441753387, 'Total Loss': 0.19820979237556458}\n",
      "Epoch 436/600, Batch 1, Losses: {'Reconstruction Loss': 0.051704809069633484, 'KL Loss': 0.14652954041957855, 'Total Loss': 0.19823434948921204}\n",
      "Epoch 437/600, Batch 1, Losses: {'Reconstruction Loss': 0.05170457437634468, 'KL Loss': 0.1468091607093811, 'Total Loss': 0.19851373136043549}\n",
      "Epoch 438/600, Batch 1, Losses: {'Reconstruction Loss': 0.052004244178533554, 'KL Loss': 0.1463310271501541, 'Total Loss': 0.19833527505397797}\n",
      "Epoch 439/600, Batch 1, Losses: {'Reconstruction Loss': 0.05174209550023079, 'KL Loss': 0.14482957124710083, 'Total Loss': 0.19657166302204132}\n",
      "Epoch 440/600, Batch 1, Losses: {'Reconstruction Loss': 0.05195525288581848, 'KL Loss': 0.14297515153884888, 'Total Loss': 0.19493040442466736}\n",
      "Epoch 441/600, Batch 1, Losses: {'Reconstruction Loss': 0.051367636770009995, 'KL Loss': 0.1426677256822586, 'Total Loss': 0.1940353661775589}\n",
      "Epoch 442/600, Batch 1, Losses: {'Reconstruction Loss': 0.05193716660141945, 'KL Loss': 0.14186899363994598, 'Total Loss': 0.19380615651607513}\n",
      "Epoch 443/600, Batch 1, Losses: {'Reconstruction Loss': 0.051658712327480316, 'KL Loss': 0.1403646618127823, 'Total Loss': 0.192023366689682}\n",
      "Epoch 444/600, Batch 1, Losses: {'Reconstruction Loss': 0.05198119208216667, 'KL Loss': 0.1425257921218872, 'Total Loss': 0.19450698792934418}\n",
      "Epoch 445/600, Batch 1, Losses: {'Reconstruction Loss': 0.05164560675621033, 'KL Loss': 0.13896287977695465, 'Total Loss': 0.19060848653316498}\n",
      "Epoch 446/600, Batch 1, Losses: {'Reconstruction Loss': 0.051821038126945496, 'KL Loss': 0.1413973867893219, 'Total Loss': 0.1932184249162674}\n",
      "Epoch 447/600, Batch 1, Losses: {'Reconstruction Loss': 0.051604218780994415, 'KL Loss': 0.1396225392818451, 'Total Loss': 0.1912267506122589}\n",
      "Epoch 448/600, Batch 1, Losses: {'Reconstruction Loss': 0.05238291993737221, 'KL Loss': 0.14006389677524567, 'Total Loss': 0.19244681298732758}\n",
      "Epoch 449/600, Batch 1, Losses: {'Reconstruction Loss': 0.051891714334487915, 'KL Loss': 0.13887974619865417, 'Total Loss': 0.1907714605331421}\n",
      "Epoch 450/600, Batch 1, Losses: {'Reconstruction Loss': 0.05158465728163719, 'KL Loss': 0.13646021485328674, 'Total Loss': 0.18804487586021423}\n",
      "Epoch 451/600, Batch 1, Losses: {'Reconstruction Loss': 0.0516628734767437, 'KL Loss': 0.13819509744644165, 'Total Loss': 0.18985797464847565}\n",
      "Epoch 452/600, Batch 1, Losses: {'Reconstruction Loss': 0.05137667432427406, 'KL Loss': 0.13524547219276428, 'Total Loss': 0.18662214279174805}\n",
      "Epoch 453/600, Batch 1, Losses: {'Reconstruction Loss': 0.05151253566145897, 'KL Loss': 0.13700217008590698, 'Total Loss': 0.18851470947265625}\n",
      "Epoch 454/600, Batch 1, Losses: {'Reconstruction Loss': 0.051976922899484634, 'KL Loss': 0.1333397477865219, 'Total Loss': 0.18531666696071625}\n",
      "Epoch 455/600, Batch 1, Losses: {'Reconstruction Loss': 0.05214067921042442, 'KL Loss': 0.13567934930324554, 'Total Loss': 0.18782003223896027}\n",
      "Epoch 456/600, Batch 1, Losses: {'Reconstruction Loss': 0.05140029266476631, 'KL Loss': 0.13381965458393097, 'Total Loss': 0.18521994352340698}\n",
      "Epoch 457/600, Batch 1, Losses: {'Reconstruction Loss': 0.05176233872771263, 'KL Loss': 0.13122215867042542, 'Total Loss': 0.18298450112342834}\n",
      "Epoch 458/600, Batch 1, Losses: {'Reconstruction Loss': 0.05118466913700104, 'KL Loss': 0.13128617405891418, 'Total Loss': 0.18247084319591522}\n",
      "Epoch 459/600, Batch 1, Losses: {'Reconstruction Loss': 0.051397956907749176, 'KL Loss': 0.13098935782909393, 'Total Loss': 0.1823873221874237}\n",
      "Epoch 460/600, Batch 1, Losses: {'Reconstruction Loss': 0.051317207515239716, 'KL Loss': 0.13056056201457977, 'Total Loss': 0.1818777620792389}\n",
      "Epoch 461/600, Batch 1, Losses: {'Reconstruction Loss': 0.051698122173547745, 'KL Loss': 0.1310015171766281, 'Total Loss': 0.18269963562488556}\n",
      "Epoch 462/600, Batch 1, Losses: {'Reconstruction Loss': 0.051326192915439606, 'KL Loss': 0.12999379634857178, 'Total Loss': 0.1813199818134308}\n",
      "Epoch 463/600, Batch 1, Losses: {'Reconstruction Loss': 0.05171613395214081, 'KL Loss': 0.12978193163871765, 'Total Loss': 0.18149806559085846}\n",
      "Epoch 464/600, Batch 1, Losses: {'Reconstruction Loss': 0.051516879349946976, 'KL Loss': 0.1279444396495819, 'Total Loss': 0.1794613152742386}\n",
      "Epoch 465/600, Batch 1, Losses: {'Reconstruction Loss': 0.05174637958407402, 'KL Loss': 0.12783025205135345, 'Total Loss': 0.17957663536071777}\n",
      "Epoch 466/600, Batch 1, Losses: {'Reconstruction Loss': 0.05114292353391647, 'KL Loss': 0.12655198574066162, 'Total Loss': 0.1776949167251587}\n",
      "Epoch 467/600, Batch 1, Losses: {'Reconstruction Loss': 0.0512576699256897, 'KL Loss': 0.12645067274570465, 'Total Loss': 0.17770834267139435}\n",
      "Epoch 468/600, Batch 1, Losses: {'Reconstruction Loss': 0.05129401013255119, 'KL Loss': 0.12353984266519547, 'Total Loss': 0.17483384907245636}\n",
      "Epoch 469/600, Batch 1, Losses: {'Reconstruction Loss': 0.0517340786755085, 'KL Loss': 0.12520171701908112, 'Total Loss': 0.17693579196929932}\n",
      "Epoch 470/600, Batch 1, Losses: {'Reconstruction Loss': 0.05155360326170921, 'KL Loss': 0.12419556081295013, 'Total Loss': 0.17574916779994965}\n",
      "Epoch 471/600, Batch 1, Losses: {'Reconstruction Loss': 0.051436878740787506, 'KL Loss': 0.1243760883808136, 'Total Loss': 0.1758129596710205}\n",
      "Epoch 472/600, Batch 1, Losses: {'Reconstruction Loss': 0.05169323831796646, 'KL Loss': 0.12185881286859512, 'Total Loss': 0.17355205118656158}\n",
      "Epoch 473/600, Batch 1, Losses: {'Reconstruction Loss': 0.05184539780020714, 'KL Loss': 0.12362628430128098, 'Total Loss': 0.17547167837619781}\n",
      "Epoch 474/600, Batch 1, Losses: {'Reconstruction Loss': 0.0520806685090065, 'KL Loss': 0.12296688556671143, 'Total Loss': 0.17504754662513733}\n",
      "Epoch 475/600, Batch 1, Losses: {'Reconstruction Loss': 0.05170055478811264, 'KL Loss': 0.1217254176735878, 'Total Loss': 0.17342597246170044}\n",
      "Epoch 476/600, Batch 1, Losses: {'Reconstruction Loss': 0.05128311738371849, 'KL Loss': 0.11953546106815338, 'Total Loss': 0.17081858217716217}\n",
      "Epoch 477/600, Batch 1, Losses: {'Reconstruction Loss': 0.0513014942407608, 'KL Loss': 0.1192205622792244, 'Total Loss': 0.1705220639705658}\n",
      "Epoch 478/600, Batch 1, Losses: {'Reconstruction Loss': 0.05143801122903824, 'KL Loss': 0.12110929191112518, 'Total Loss': 0.17254731059074402}\n",
      "Epoch 479/600, Batch 1, Losses: {'Reconstruction Loss': 0.05171649530529976, 'KL Loss': 0.12120926380157471, 'Total Loss': 0.17292575538158417}\n",
      "Epoch 480/600, Batch 1, Losses: {'Reconstruction Loss': 0.05146476998925209, 'KL Loss': 0.11945953965187073, 'Total Loss': 0.17092430591583252}\n",
      "Epoch 481/600, Batch 1, Losses: {'Reconstruction Loss': 0.051934465765953064, 'KL Loss': 0.12057177722454071, 'Total Loss': 0.17250624299049377}\n",
      "Epoch 482/600, Batch 1, Losses: {'Reconstruction Loss': 0.05217832326889038, 'KL Loss': 0.12123868614435196, 'Total Loss': 0.17341700196266174}\n",
      "Epoch 483/600, Batch 1, Losses: {'Reconstruction Loss': 0.051678404211997986, 'KL Loss': 0.11859282851219177, 'Total Loss': 0.17027123272418976}\n",
      "Epoch 484/600, Batch 1, Losses: {'Reconstruction Loss': 0.05174776911735535, 'KL Loss': 0.11775333434343338, 'Total Loss': 0.16950109601020813}\n",
      "Epoch 485/600, Batch 1, Losses: {'Reconstruction Loss': 0.05147522687911987, 'KL Loss': 0.11713167279958725, 'Total Loss': 0.16860690712928772}\n",
      "Epoch 486/600, Batch 1, Losses: {'Reconstruction Loss': 0.05165242776274681, 'KL Loss': 0.11655682325363159, 'Total Loss': 0.1682092547416687}\n",
      "Epoch 487/600, Batch 1, Losses: {'Reconstruction Loss': 0.05125102028250694, 'KL Loss': 0.11394817382097244, 'Total Loss': 0.1651991903781891}\n",
      "Epoch 488/600, Batch 1, Losses: {'Reconstruction Loss': 0.05178157985210419, 'KL Loss': 0.11544221639633179, 'Total Loss': 0.16722379624843597}\n",
      "Epoch 489/600, Batch 1, Losses: {'Reconstruction Loss': 0.05119709670543671, 'KL Loss': 0.11491077393293381, 'Total Loss': 0.16610786318778992}\n",
      "Epoch 490/600, Batch 1, Losses: {'Reconstruction Loss': 0.051667846739292145, 'KL Loss': 0.11592031270265579, 'Total Loss': 0.16758815944194794}\n",
      "Epoch 491/600, Batch 1, Losses: {'Reconstruction Loss': 0.05179586261510849, 'KL Loss': 0.11415200680494308, 'Total Loss': 0.16594786942005157}\n",
      "Epoch 492/600, Batch 1, Losses: {'Reconstruction Loss': 0.05131005868315697, 'KL Loss': 0.11491064727306366, 'Total Loss': 0.16622070968151093}\n",
      "Epoch 493/600, Batch 1, Losses: {'Reconstruction Loss': 0.05147871747612953, 'KL Loss': 0.11292614787817001, 'Total Loss': 0.16440486907958984}\n",
      "Epoch 494/600, Batch 1, Losses: {'Reconstruction Loss': 0.05145353451371193, 'KL Loss': 0.11321920901536942, 'Total Loss': 0.16467274725437164}\n",
      "Epoch 495/600, Batch 1, Losses: {'Reconstruction Loss': 0.05147625133395195, 'KL Loss': 0.11329605430364609, 'Total Loss': 0.16477230191230774}\n",
      "Epoch 496/600, Batch 1, Losses: {'Reconstruction Loss': 0.051162417978048325, 'KL Loss': 0.11113151907920837, 'Total Loss': 0.162293940782547}\n",
      "Epoch 497/600, Batch 1, Losses: {'Reconstruction Loss': 0.051140349358320236, 'KL Loss': 0.10967173427343369, 'Total Loss': 0.16081207990646362}\n",
      "Epoch 498/600, Batch 1, Losses: {'Reconstruction Loss': 0.05138545110821724, 'KL Loss': 0.11157433688640594, 'Total Loss': 0.16295978426933289}\n",
      "Epoch 499/600, Batch 1, Losses: {'Reconstruction Loss': 0.05126125365495682, 'KL Loss': 0.10979296267032623, 'Total Loss': 0.16105422377586365}\n",
      "Epoch 500/600, Batch 1, Losses: {'Reconstruction Loss': 0.05157523229718208, 'KL Loss': 0.10885641723871231, 'Total Loss': 0.1604316532611847}\n",
      "Epoch 501/600, Batch 1, Losses: {'Reconstruction Loss': 0.05208072438836098, 'KL Loss': 0.10939011722803116, 'Total Loss': 0.16147084534168243}\n",
      "Epoch 502/600, Batch 1, Losses: {'Reconstruction Loss': 0.05199044942855835, 'KL Loss': 0.11064469069242477, 'Total Loss': 0.16263514757156372}\n",
      "Epoch 503/600, Batch 1, Losses: {'Reconstruction Loss': 0.051636870950460434, 'KL Loss': 0.10891398787498474, 'Total Loss': 0.16055086255073547}\n",
      "Epoch 504/600, Batch 1, Losses: {'Reconstruction Loss': 0.05150988698005676, 'KL Loss': 0.10724571347236633, 'Total Loss': 0.1587556004524231}\n",
      "Epoch 505/600, Batch 1, Losses: {'Reconstruction Loss': 0.05145544931292534, 'KL Loss': 0.10842537879943848, 'Total Loss': 0.1598808318376541}\n",
      "Epoch 506/600, Batch 1, Losses: {'Reconstruction Loss': 0.05161517858505249, 'KL Loss': 0.10632352530956268, 'Total Loss': 0.15793870389461517}\n",
      "Epoch 507/600, Batch 1, Losses: {'Reconstruction Loss': 0.051410965621471405, 'KL Loss': 0.10632538050413132, 'Total Loss': 0.15773634612560272}\n",
      "Epoch 508/600, Batch 1, Losses: {'Reconstruction Loss': 0.05145418643951416, 'KL Loss': 0.10513494908809662, 'Total Loss': 0.15658913552761078}\n",
      "Epoch 509/600, Batch 1, Losses: {'Reconstruction Loss': 0.05176399275660515, 'KL Loss': 0.10492829233407974, 'Total Loss': 0.1566922813653946}\n",
      "Epoch 510/600, Batch 1, Losses: {'Reconstruction Loss': 0.05166924372315407, 'KL Loss': 0.10437770932912827, 'Total Loss': 0.15604695677757263}\n",
      "Epoch 511/600, Batch 1, Losses: {'Reconstruction Loss': 0.051479876041412354, 'KL Loss': 0.10488804429769516, 'Total Loss': 0.1563679277896881}\n",
      "Epoch 512/600, Batch 1, Losses: {'Reconstruction Loss': 0.05166410654783249, 'KL Loss': 0.10322917252779007, 'Total Loss': 0.15489327907562256}\n",
      "Epoch 513/600, Batch 1, Losses: {'Reconstruction Loss': 0.05133098363876343, 'KL Loss': 0.10117980092763901, 'Total Loss': 0.15251079201698303}\n",
      "Epoch 514/600, Batch 1, Losses: {'Reconstruction Loss': 0.0517745167016983, 'KL Loss': 0.10245092958211899, 'Total Loss': 0.1542254388332367}\n",
      "Epoch 515/600, Batch 1, Losses: {'Reconstruction Loss': 0.05151427909731865, 'KL Loss': 0.10242535173892975, 'Total Loss': 0.1539396345615387}\n",
      "Epoch 516/600, Batch 1, Losses: {'Reconstruction Loss': 0.051838699728250504, 'KL Loss': 0.10311281681060791, 'Total Loss': 0.15495151281356812}\n",
      "Epoch 517/600, Batch 1, Losses: {'Reconstruction Loss': 0.051486704498529434, 'KL Loss': 0.10178092122077942, 'Total Loss': 0.15326762199401855}\n",
      "Epoch 518/600, Batch 1, Losses: {'Reconstruction Loss': 0.0516691654920578, 'KL Loss': 0.10149906575679779, 'Total Loss': 0.1531682312488556}\n",
      "Epoch 519/600, Batch 1, Losses: {'Reconstruction Loss': 0.05142832174897194, 'KL Loss': 0.09970720112323761, 'Total Loss': 0.15113551914691925}\n",
      "Epoch 520/600, Batch 1, Losses: {'Reconstruction Loss': 0.05166172608733177, 'KL Loss': 0.10103634744882584, 'Total Loss': 0.1526980698108673}\n",
      "Epoch 521/600, Batch 1, Losses: {'Reconstruction Loss': 0.05178594961762428, 'KL Loss': 0.10087992995977402, 'Total Loss': 0.1526658833026886}\n",
      "Epoch 522/600, Batch 1, Losses: {'Reconstruction Loss': 0.05134356766939163, 'KL Loss': 0.10061639547348022, 'Total Loss': 0.15195995569229126}\n",
      "Epoch 523/600, Batch 1, Losses: {'Reconstruction Loss': 0.051296934485435486, 'KL Loss': 0.09954162687063217, 'Total Loss': 0.15083855390548706}\n",
      "Epoch 524/600, Batch 1, Losses: {'Reconstruction Loss': 0.051846764981746674, 'KL Loss': 0.09823493659496307, 'Total Loss': 0.15008169412612915}\n",
      "Epoch 525/600, Batch 1, Losses: {'Reconstruction Loss': 0.05218232050538063, 'KL Loss': 0.0993015393614769, 'Total Loss': 0.15148386359214783}\n",
      "Epoch 526/600, Batch 1, Losses: {'Reconstruction Loss': 0.05176323652267456, 'KL Loss': 0.09835442155599594, 'Total Loss': 0.1501176655292511}\n",
      "Epoch 527/600, Batch 1, Losses: {'Reconstruction Loss': 0.051339950412511826, 'KL Loss': 0.09822821617126465, 'Total Loss': 0.14956817030906677}\n",
      "Epoch 528/600, Batch 1, Losses: {'Reconstruction Loss': 0.05167797580361366, 'KL Loss': 0.09787642955780029, 'Total Loss': 0.14955440163612366}\n",
      "Epoch 529/600, Batch 1, Losses: {'Reconstruction Loss': 0.05160247161984444, 'KL Loss': 0.09768693894147873, 'Total Loss': 0.14928941428661346}\n",
      "Epoch 530/600, Batch 1, Losses: {'Reconstruction Loss': 0.05175318568944931, 'KL Loss': 0.09645413607358932, 'Total Loss': 0.14820732176303864}\n",
      "Epoch 531/600, Batch 1, Losses: {'Reconstruction Loss': 0.05159978196024895, 'KL Loss': 0.09547743201255798, 'Total Loss': 0.14707721769809723}\n",
      "Epoch 532/600, Batch 1, Losses: {'Reconstruction Loss': 0.05156690627336502, 'KL Loss': 0.0954369381070137, 'Total Loss': 0.14700384438037872}\n",
      "Epoch 533/600, Batch 1, Losses: {'Reconstruction Loss': 0.051371145993471146, 'KL Loss': 0.093864805996418, 'Total Loss': 0.14523595571517944}\n",
      "Epoch 534/600, Batch 1, Losses: {'Reconstruction Loss': 0.051276881247758865, 'KL Loss': 0.09408720582723618, 'Total Loss': 0.14536409080028534}\n",
      "Epoch 535/600, Batch 1, Losses: {'Reconstruction Loss': 0.05199442431330681, 'KL Loss': 0.09523837268352509, 'Total Loss': 0.1472328007221222}\n",
      "Epoch 536/600, Batch 1, Losses: {'Reconstruction Loss': 0.05179038643836975, 'KL Loss': 0.09498005360364914, 'Total Loss': 0.1467704474925995}\n",
      "Epoch 537/600, Batch 1, Losses: {'Reconstruction Loss': 0.05126695707440376, 'KL Loss': 0.09228286147117615, 'Total Loss': 0.1435498148202896}\n",
      "Epoch 538/600, Batch 1, Losses: {'Reconstruction Loss': 0.05163272097706795, 'KL Loss': 0.09392403811216354, 'Total Loss': 0.1455567628145218}\n",
      "Epoch 539/600, Batch 1, Losses: {'Reconstruction Loss': 0.051387008279561996, 'KL Loss': 0.0922117680311203, 'Total Loss': 0.1435987800359726}\n",
      "Epoch 540/600, Batch 1, Losses: {'Reconstruction Loss': 0.05117752030491829, 'KL Loss': 0.09165674448013306, 'Total Loss': 0.14283426105976105}\n",
      "Epoch 541/600, Batch 1, Losses: {'Reconstruction Loss': 0.05133882910013199, 'KL Loss': 0.09177816659212112, 'Total Loss': 0.1431169956922531}\n",
      "Epoch 542/600, Batch 1, Losses: {'Reconstruction Loss': 0.05163191258907318, 'KL Loss': 0.09136009216308594, 'Total Loss': 0.14299200475215912}\n",
      "Epoch 543/600, Batch 1, Losses: {'Reconstruction Loss': 0.05159740895032883, 'KL Loss': 0.09099084138870239, 'Total Loss': 0.14258825778961182}\n",
      "Epoch 544/600, Batch 1, Losses: {'Reconstruction Loss': 0.05174451321363449, 'KL Loss': 0.08953602612018585, 'Total Loss': 0.14128053188323975}\n",
      "Epoch 545/600, Batch 1, Losses: {'Reconstruction Loss': 0.0514921098947525, 'KL Loss': 0.09015706926584244, 'Total Loss': 0.14164918661117554}\n",
      "Epoch 546/600, Batch 1, Losses: {'Reconstruction Loss': 0.051654789596796036, 'KL Loss': 0.08940883725881577, 'Total Loss': 0.1410636305809021}\n",
      "Epoch 547/600, Batch 1, Losses: {'Reconstruction Loss': 0.05156839266419411, 'KL Loss': 0.08986259251832962, 'Total Loss': 0.14143098890781403}\n",
      "Epoch 548/600, Batch 1, Losses: {'Reconstruction Loss': 0.05139685422182083, 'KL Loss': 0.08911605179309845, 'Total Loss': 0.14051291346549988}\n",
      "Epoch 549/600, Batch 1, Losses: {'Reconstruction Loss': 0.05170898139476776, 'KL Loss': 0.08876565843820572, 'Total Loss': 0.14047464728355408}\n",
      "Epoch 550/600, Batch 1, Losses: {'Reconstruction Loss': 0.051985494792461395, 'KL Loss': 0.08896061033010483, 'Total Loss': 0.14094610512256622}\n",
      "Epoch 551/600, Batch 1, Losses: {'Reconstruction Loss': 0.05149342492222786, 'KL Loss': 0.08909451216459274, 'Total Loss': 0.1405879408121109}\n",
      "Epoch 552/600, Batch 1, Losses: {'Reconstruction Loss': 0.051646240055561066, 'KL Loss': 0.08792973309755325, 'Total Loss': 0.13957597315311432}\n",
      "Epoch 553/600, Batch 1, Losses: {'Reconstruction Loss': 0.051318712532520294, 'KL Loss': 0.08745674788951874, 'Total Loss': 0.13877546787261963}\n",
      "Epoch 554/600, Batch 1, Losses: {'Reconstruction Loss': 0.051046065986156464, 'KL Loss': 0.08720337599515915, 'Total Loss': 0.1382494419813156}\n",
      "Epoch 555/600, Batch 1, Losses: {'Reconstruction Loss': 0.051666781306266785, 'KL Loss': 0.08564434200525284, 'Total Loss': 0.13731113076210022}\n",
      "Epoch 556/600, Batch 1, Losses: {'Reconstruction Loss': 0.05159032344818115, 'KL Loss': 0.08637681603431702, 'Total Loss': 0.13796713948249817}\n",
      "Epoch 557/600, Batch 1, Losses: {'Reconstruction Loss': 0.05099841207265854, 'KL Loss': 0.08477562665939331, 'Total Loss': 0.13577404618263245}\n",
      "Epoch 558/600, Batch 1, Losses: {'Reconstruction Loss': 0.05106889083981514, 'KL Loss': 0.08506336808204651, 'Total Loss': 0.13613225519657135}\n",
      "Epoch 559/600, Batch 1, Losses: {'Reconstruction Loss': 0.05139419436454773, 'KL Loss': 0.08507828414440155, 'Total Loss': 0.13647247850894928}\n",
      "Epoch 560/600, Batch 1, Losses: {'Reconstruction Loss': 0.0510336197912693, 'KL Loss': 0.08429892361164093, 'Total Loss': 0.13533253967761993}\n",
      "Epoch 561/600, Batch 1, Losses: {'Reconstruction Loss': 0.051695890724658966, 'KL Loss': 0.08469673991203308, 'Total Loss': 0.13639262318611145}\n",
      "Epoch 562/600, Batch 1, Losses: {'Reconstruction Loss': 0.05164700001478195, 'KL Loss': 0.08484587073326111, 'Total Loss': 0.13649287819862366}\n",
      "Epoch 563/600, Batch 1, Losses: {'Reconstruction Loss': 0.05163991078734398, 'KL Loss': 0.08530858904123306, 'Total Loss': 0.13694849610328674}\n",
      "Epoch 564/600, Batch 1, Losses: {'Reconstruction Loss': 0.05114548280835152, 'KL Loss': 0.08412580192089081, 'Total Loss': 0.13527128100395203}\n",
      "Epoch 565/600, Batch 1, Losses: {'Reconstruction Loss': 0.051453616470098495, 'KL Loss': 0.08337877690792084, 'Total Loss': 0.13483239710330963}\n",
      "Epoch 566/600, Batch 1, Losses: {'Reconstruction Loss': 0.05156543105840683, 'KL Loss': 0.08427140861749649, 'Total Loss': 0.13583683967590332}\n",
      "Epoch 567/600, Batch 1, Losses: {'Reconstruction Loss': 0.05163314938545227, 'KL Loss': 0.08277823030948639, 'Total Loss': 0.13441137969493866}\n",
      "Epoch 568/600, Batch 1, Losses: {'Reconstruction Loss': 0.051523275673389435, 'KL Loss': 0.08398190140724182, 'Total Loss': 0.13550516963005066}\n",
      "Epoch 569/600, Batch 1, Losses: {'Reconstruction Loss': 0.051804207265377045, 'KL Loss': 0.08231120556592941, 'Total Loss': 0.13411541283130646}\n",
      "Epoch 570/600, Batch 1, Losses: {'Reconstruction Loss': 0.051285792142152786, 'KL Loss': 0.08170120418071747, 'Total Loss': 0.13298699259757996}\n",
      "Epoch 571/600, Batch 1, Losses: {'Reconstruction Loss': 0.051302868872880936, 'KL Loss': 0.0805843397974968, 'Total Loss': 0.13188721239566803}\n",
      "Epoch 572/600, Batch 1, Losses: {'Reconstruction Loss': 0.051699332892894745, 'KL Loss': 0.08186972141265869, 'Total Loss': 0.13356906175613403}\n",
      "Epoch 573/600, Batch 1, Losses: {'Reconstruction Loss': 0.051651809364557266, 'KL Loss': 0.08119886368513107, 'Total Loss': 0.13285067677497864}\n",
      "Epoch 574/600, Batch 1, Losses: {'Reconstruction Loss': 0.05103639140725136, 'KL Loss': 0.0804772675037384, 'Total Loss': 0.13151365518569946}\n",
      "Epoch 575/600, Batch 1, Losses: {'Reconstruction Loss': 0.05097896233201027, 'KL Loss': 0.07914616912603378, 'Total Loss': 0.13012513518333435}\n",
      "Epoch 576/600, Batch 1, Losses: {'Reconstruction Loss': 0.051540557295084, 'KL Loss': 0.08110684901475906, 'Total Loss': 0.13264741003513336}\n",
      "Epoch 577/600, Batch 1, Losses: {'Reconstruction Loss': 0.05177213251590729, 'KL Loss': 0.07937151938676834, 'Total Loss': 0.13114365935325623}\n",
      "Epoch 578/600, Batch 1, Losses: {'Reconstruction Loss': 0.05193401873111725, 'KL Loss': 0.07961603999137878, 'Total Loss': 0.13155005872249603}\n",
      "Epoch 579/600, Batch 1, Losses: {'Reconstruction Loss': 0.05178855359554291, 'KL Loss': 0.07886739820241928, 'Total Loss': 0.1306559443473816}\n",
      "Epoch 580/600, Batch 1, Losses: {'Reconstruction Loss': 0.051637403666973114, 'KL Loss': 0.07799600064754486, 'Total Loss': 0.12963339686393738}\n",
      "Epoch 581/600, Batch 1, Losses: {'Reconstruction Loss': 0.05158897116780281, 'KL Loss': 0.07827731966972351, 'Total Loss': 0.12986628711223602}\n",
      "Epoch 582/600, Batch 1, Losses: {'Reconstruction Loss': 0.051502302289009094, 'KL Loss': 0.07752054929733276, 'Total Loss': 0.12902285158634186}\n",
      "Epoch 583/600, Batch 1, Losses: {'Reconstruction Loss': 0.05126229673624039, 'KL Loss': 0.07653466612100601, 'Total Loss': 0.1277969628572464}\n",
      "Epoch 584/600, Batch 1, Losses: {'Reconstruction Loss': 0.051308438181877136, 'KL Loss': 0.07783372700214386, 'Total Loss': 0.129142165184021}\n",
      "Epoch 585/600, Batch 1, Losses: {'Reconstruction Loss': 0.0513935312628746, 'KL Loss': 0.0761122927069664, 'Total Loss': 0.127505823969841}\n",
      "Epoch 586/600, Batch 1, Losses: {'Reconstruction Loss': 0.05160561949014664, 'KL Loss': 0.07693874835968018, 'Total Loss': 0.12854436039924622}\n",
      "Epoch 587/600, Batch 1, Losses: {'Reconstruction Loss': 0.05158295854926109, 'KL Loss': 0.0769813284277916, 'Total Loss': 0.1285642832517624}\n",
      "Epoch 588/600, Batch 1, Losses: {'Reconstruction Loss': 0.051278144121170044, 'KL Loss': 0.07651413232088089, 'Total Loss': 0.12779226899147034}\n",
      "Epoch 589/600, Batch 1, Losses: {'Reconstruction Loss': 0.05146467313170433, 'KL Loss': 0.07595591992139816, 'Total Loss': 0.1274205893278122}\n",
      "Epoch 590/600, Batch 1, Losses: {'Reconstruction Loss': 0.05164604261517525, 'KL Loss': 0.07569200545549393, 'Total Loss': 0.12733805179595947}\n",
      "Epoch 591/600, Batch 1, Losses: {'Reconstruction Loss': 0.05145366117358208, 'KL Loss': 0.07564198225736618, 'Total Loss': 0.12709563970565796}\n",
      "Epoch 592/600, Batch 1, Losses: {'Reconstruction Loss': 0.05176698416471481, 'KL Loss': 0.07544989138841629, 'Total Loss': 0.1272168755531311}\n",
      "Epoch 593/600, Batch 1, Losses: {'Reconstruction Loss': 0.0515778511762619, 'KL Loss': 0.07545290142297745, 'Total Loss': 0.12703076004981995}\n",
      "Epoch 594/600, Batch 1, Losses: {'Reconstruction Loss': 0.05120518431067467, 'KL Loss': 0.07316949218511581, 'Total Loss': 0.12437467277050018}\n",
      "Epoch 595/600, Batch 1, Losses: {'Reconstruction Loss': 0.05160801485180855, 'KL Loss': 0.07293372601270676, 'Total Loss': 0.1245417445898056}\n",
      "Epoch 596/600, Batch 1, Losses: {'Reconstruction Loss': 0.05149869993329048, 'KL Loss': 0.07307878136634827, 'Total Loss': 0.12457747757434845}\n",
      "Epoch 597/600, Batch 1, Losses: {'Reconstruction Loss': 0.05124391242861748, 'KL Loss': 0.07349559664726257, 'Total Loss': 0.12473951280117035}\n",
      "Epoch 598/600, Batch 1, Losses: {'Reconstruction Loss': 0.051926009356975555, 'KL Loss': 0.07404469698667526, 'Total Loss': 0.12597070634365082}\n",
      "Epoch 599/600, Batch 1, Losses: {'Reconstruction Loss': 0.051449764519929886, 'KL Loss': 0.07273820042610168, 'Total Loss': 0.12418796122074127}\n",
      "Epoch 600/600, Batch 1, Losses: {'Reconstruction Loss': 0.05168662220239639, 'KL Loss': 0.07258640974760056, 'Total Loss': 0.12427303194999695}\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 70  # Latent dimension\n",
    "data_dim = 100  # Data dimension of input\n",
    "hidden_dim = 512\n",
    "n_layers = 8\n",
    "latent_diagonal = prior_eigenvalues[:latent_dim]  # Eigenvalues for latent prior\n",
    "batch_size = 1000  # Batch size for training\n",
    "beta_ = 1.0  # Beta value for beta-VAE\n",
    "pre_train_learning_rate = 1e-3\n",
    "fine_tune_learning_rate = 1e-4  # Fine-tune learning rate\n",
    "pre_train_epochs = 600  # Number of pre-train epochs\n",
    "fine_tune_epochs = 20  # Number of fine-tune epochs\n",
    "device = \"cpu\"  # Use CPU as the device\n",
    "\n",
    "trainer = SurfaceVAETrainer(\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_layers=n_layers,\n",
    "    data_dim=data_dim,\n",
    "    latent_diagonal=latent_diagonal,\n",
    "    batch_size=batch_size,\n",
    "    beta=beta_,\n",
    "    pre_train_learning_rate=pre_train_learning_rate,\n",
    "    fine_tune_learning_rate=fine_tune_learning_rate,\n",
    "    pre_train_epochs=pre_train_epochs,\n",
    "    fine_tune_epochs=fine_tune_epochs,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Train the model using pre_train\n",
    "trainer.pre_train_with_sampling(\n",
    "    smoothness_prior=smoothness_prior,\n",
    "    experiment_name=\"test vae\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Reconstruction Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599
         ],
         "xaxis": "x",
         "y": [
          6.999632358551025,
          3.442704200744629,
          1.7724372148513794,
          0.9979664087295532,
          0.47238582372665405,
          0.5227526426315308,
          1.4964783191680908,
          1.5469316244125366,
          1.8836926221847534,
          1.7137385606765747,
          1.0780318975448608,
          1.5972115993499756,
          1.0208817720413208,
          0.5973833799362183,
          0.7869369983673096,
          0.6849003434181213,
          0.3617017865180969,
          0.36649882793426514,
          0.3102552592754364,
          0.5103885531425476,
          0.6276708245277405,
          0.45052650570869446,
          0.23850463330745697,
          0.2557494342327118,
          0.1814538836479187,
          0.15063107013702393,
          0.20960748195648193,
          0.41195279359817505,
          0.37263864278793335,
          0.1771387755870819,
          0.22964543104171753,
          0.2903531491756439,
          0.3367882966995239,
          0.22614163160324097,
          0.22190655767917633,
          0.239854633808136,
          0.1716460883617401,
          0.1617700159549713,
          0.24857474863529205,
          0.2911030054092407,
          0.23773787915706635,
          0.19772300124168396,
          0.25813740491867065,
          0.29957231879234314,
          0.5078401565551758,
          0.6006034016609192,
          0.3756084144115448,
          0.2815382480621338,
          0.38446348905563354,
          0.4506106376647949,
          0.6111804246902466,
          0.6496163606643677,
          0.4026774764060974,
          0.3779488205909729,
          0.34938716888427734,
          0.25741562247276306,
          0.3012462854385376,
          0.37597060203552246,
          0.2715337574481964,
          0.17991584539413452,
          0.2225385308265686,
          0.24440881609916687,
          0.23979584872722626,
          0.21264049410820007,
          0.1657574623823166,
          0.1235867440700531,
          0.12441643327474594,
          0.15108652412891388,
          0.17644673585891724,
          0.1714833378791809,
          0.14280597865581512,
          0.11645819991827011,
          0.11721032112836838,
          0.1311390995979309,
          0.13678139448165894,
          0.12748216092586517,
          0.10194758325815201,
          0.0884929671883583,
          0.09690950065851212,
          0.10692865401506424,
          0.10418123751878738,
          0.09352165460586548,
          0.08772700279951096,
          0.08573894202709198,
          0.08144325762987137,
          0.07650242745876312,
          0.07377280294895172,
          0.07520793378353119,
          0.07808583229780197,
          0.07775605469942093,
          0.07335259765386581,
          0.06837957352399826,
          0.06687179207801819,
          0.06820470094680786,
          0.06869925558567047,
          0.06764832884073257,
          0.06394969671964645,
          0.0613555908203125,
          0.061166178435087204,
          0.0615219920873642,
          0.0625205710530281,
          0.06177402287721634,
          0.059738025069236755,
          0.05914369225502014,
          0.057973168790340424,
          0.05838803946971893,
          0.05912649631500244,
          0.05885438621044159,
          0.05912213400006294,
          0.05795422941446304,
          0.0569278821349144,
          0.05648212134838104,
          0.05661044642329216,
          0.056668251752853394,
          0.0563543327152729,
          0.056625887751579285,
          0.056294746696949005,
          0.05503176525235176,
          0.05544617399573326,
          0.05542523041367531,
          0.05538976937532425,
          0.05586910992860794,
          0.05529187619686127,
          0.05493566766381264,
          0.05490168184041977,
          0.05497139319777489,
          0.05516162887215614,
          0.054820913821458817,
          0.05506893992424011,
          0.05451617017388344,
          0.05465470626950264,
          0.054717399179935455,
          0.05406605079770088,
          0.05377986282110214,
          0.05400735139846802,
          0.05390825495123863,
          0.05398481711745262,
          0.05421571061015129,
          0.054007500410079956,
          0.05380823090672493,
          0.05358937010169029,
          0.05361084267497063,
          0.05361589044332504,
          0.053466979414224625,
          0.05418084189295769,
          0.05342715233564377,
          0.05313275381922722,
          0.053379468619823456,
          0.05423719808459282,
          0.054047539830207825,
          0.054095298051834106,
          0.05390122905373573,
          0.05294056609272957,
          0.05327155441045761,
          0.05344623327255249,
          0.05316226929426193,
          0.053313933312892914,
          0.05300626531243324,
          0.053451888263225555,
          0.05290660634636879,
          0.05312086641788483,
          0.052984222769737244,
          0.05317411571741104,
          0.05318852514028549,
          0.05312573164701462,
          0.05297878757119179,
          0.05297015607357025,
          0.053175896406173706,
          0.053182173520326614,
          0.05320348963141441,
          0.053286559879779816,
          0.052810944616794586,
          0.05302360653877258,
          0.05299730226397514,
          0.05305667966604233,
          0.05288085341453552,
          0.05331481993198395,
          0.05268498510122299,
          0.05290243402123451,
          0.052515219897031784,
          0.05298367887735367,
          0.052828095853328705,
          0.0533711314201355,
          0.05302887037396431,
          0.05293029546737671,
          0.05285827815532684,
          0.05271166190505028,
          0.05288132280111313,
          0.05290878191590309,
          0.052237365394830704,
          0.05231890827417374,
          0.053126730024814606,
          0.05304856598377228,
          0.052729930728673935,
          0.05239984020590782,
          0.052876587957143784,
          0.05298323184251785,
          0.05265185981988907,
          0.05199790373444557,
          0.053014032542705536,
          0.052834607660770416,
          0.05275314673781395,
          0.052421361207962036,
          0.052719902247190475,
          0.052895475178956985,
          0.0527399405837059,
          0.05249088257551193,
          0.05252188444137573,
          0.052443649619817734,
          0.052377454936504364,
          0.052572593092918396,
          0.05236088111996651,
          0.05242585018277168,
          0.05278719216585159,
          0.05218387767672539,
          0.05203401669859886,
          0.052227284759283066,
          0.05281568318605423,
          0.052291616797447205,
          0.052551474422216415,
          0.052401039749383926,
          0.052526798099279404,
          0.05258326232433319,
          0.05257998779416084,
          0.052223872393369675,
          0.05231250077486038,
          0.05282696336507797,
          0.05217644199728966,
          0.052667852491140366,
          0.0524667389690876,
          0.05241503193974495,
          0.05211413651704788,
          0.052312470972537994,
          0.05193854868412018,
          0.05253102257847786,
          0.052054498344659805,
          0.052101943641901016,
          0.05271868407726288,
          0.052456147968769073,
          0.05252270773053169,
          0.05189568176865578,
          0.05202769488096237,
          0.052241384983062744,
          0.05215398222208023,
          0.05188152939081192,
          0.05206787586212158,
          0.05237109214067459,
          0.052450649440288544,
          0.05226229131221771,
          0.0523453913629055,
          0.05231780931353569,
          0.05263247340917587,
          0.05266481637954712,
          0.051577333360910416,
          0.052348725497722626,
          0.052043356001377106,
          0.05208229646086693,
          0.05199824646115303,
          0.05196351930499077,
          0.05211829021573067,
          0.05244235321879387,
          0.05214153230190277,
          0.05262644961476326,
          0.052080292254686356,
          0.05182049423456192,
          0.05161534249782562,
          0.052477989345788956,
          0.051828935742378235,
          0.05221250653266907,
          0.052686721086502075,
          0.0519716702401638,
          0.052012767642736435,
          0.052022092044353485,
          0.05214184895157814,
          0.051818229258060455,
          0.05200597643852234,
          0.05216265097260475,
          0.05179840698838234,
          0.052268631756305695,
          0.05226705223321915,
          0.05176278203725815,
          0.05206577852368355,
          0.051981329917907715,
          0.05189226567745209,
          0.05219436064362526,
          0.05148177221417427,
          0.052282966673374176,
          0.05191320925951004,
          0.0520077720284462,
          0.051775749772787094,
          0.05212533846497536,
          0.05190557241439819,
          0.052426401525735855,
          0.05235792696475983,
          0.05196833238005638,
          0.05228961259126663,
          0.051922187209129333,
          0.0522281788289547,
          0.05208407714962959,
          0.05169825628399849,
          0.052184924483299255,
          0.051542967557907104,
          0.0518549382686615,
          0.051518507301807404,
          0.05159831792116165,
          0.05153267830610275,
          0.051740601658821106,
          0.05192165449261665,
          0.05199515074491501,
          0.05180380865931511,
          0.05140652507543564,
          0.0517011322081089,
          0.05226845666766167,
          0.05184803158044815,
          0.051815345883369446,
          0.051754243671894073,
          0.05184982344508171,
          0.052165139466524124,
          0.05225029215216637,
          0.051580075174570084,
          0.05207015946507454,
          0.05160517618060112,
          0.05203639343380928,
          0.05188717320561409,
          0.05163715034723282,
          0.05182212218642235,
          0.051472678780555725,
          0.05250904709100723,
          0.05213722586631775,
          0.05177723988890648,
          0.051624491810798645,
          0.05211792141199112,
          0.05224321782588959,
          0.051738057285547256,
          0.05206022411584854,
          0.05186805501580238,
          0.05182701349258423,
          0.051535047590732574,
          0.05148430913686752,
          0.051479797810316086,
          0.05223352089524269,
          0.05176956206560135,
          0.051722101867198944,
          0.05177579075098038,
          0.05190563201904297,
          0.0512944757938385,
          0.051190849393606186,
          0.05189388245344162,
          0.05153074860572815,
          0.051787957549095154,
          0.05177031084895134,
          0.051680564880371094,
          0.051533110439777374,
          0.05173031613230705,
          0.05175779387354851,
          0.05163382738828659,
          0.05215279012918472,
          0.05171544477343559,
          0.05209916830062866,
          0.051577046513557434,
          0.051568739116191864,
          0.05186406150460243,
          0.05162665620446205,
          0.05169733613729477,
          0.0517706461250782,
          0.05167112499475479,
          0.051600221544504166,
          0.05143299326300621,
          0.05169439688324928,
          0.051696471869945526,
          0.05185110494494438,
          0.051758453249931335,
          0.051529936492443085,
          0.05169551819562912,
          0.051305536180734634,
          0.051887765526771545,
          0.051915690302848816,
          0.05185869708657265,
          0.05161180719733238,
          0.05161825194954872,
          0.05195418372750282,
          0.0516795814037323,
          0.05171986669301987,
          0.05218181759119034,
          0.05198865756392479,
          0.05194782465696335,
          0.051493965089321136,
          0.05166001245379448,
          0.0519217923283577,
          0.05178386718034744,
          0.05176465958356857,
          0.05152025818824768,
          0.0519251748919487,
          0.05200684443116188,
          0.05127432942390442,
          0.05188547447323799,
          0.05148402228951454,
          0.05189790949225426,
          0.051954224705696106,
          0.051622193306684494,
          0.051399219781160355,
          0.05190831050276756,
          0.051801733672618866,
          0.05188921466469765,
          0.051550183445215225,
          0.05163811892271042,
          0.05158863589167595,
          0.051472257822752,
          0.051644694060087204,
          0.051741115748882294,
          0.05146436393260956,
          0.051844947040081024,
          0.051294922828674316,
          0.05220465734601021,
          0.051849886775016785,
          0.05180957168340683,
          0.05155918002128601,
          0.05191650614142418,
          0.051631368696689606,
          0.05163348838686943,
          0.05151204392313957,
          0.051569096744060516,
          0.05170580372214317,
          0.05218983441591263,
          0.051733292639255524,
          0.051785241812467575,
          0.05184269696474075,
          0.05166688561439514,
          0.05194160342216492,
          0.051774609833955765,
          0.051780715584754944,
          0.05133472755551338,
          0.051999643445014954,
          0.0513543039560318,
          0.051444150507450104,
          0.051704809069633484,
          0.05170457437634468,
          0.052004244178533554,
          0.05174209550023079,
          0.05195525288581848,
          0.051367636770009995,
          0.05193716660141945,
          0.051658712327480316,
          0.05198119208216667,
          0.05164560675621033,
          0.051821038126945496,
          0.051604218780994415,
          0.05238291993737221,
          0.051891714334487915,
          0.05158465728163719,
          0.0516628734767437,
          0.05137667432427406,
          0.05151253566145897,
          0.051976922899484634,
          0.05214067921042442,
          0.05140029266476631,
          0.05176233872771263,
          0.05118466913700104,
          0.051397956907749176,
          0.051317207515239716,
          0.051698122173547745,
          0.051326192915439606,
          0.05171613395214081,
          0.051516879349946976,
          0.05174637958407402,
          0.05114292353391647,
          0.0512576699256897,
          0.05129401013255119,
          0.0517340786755085,
          0.05155360326170921,
          0.051436878740787506,
          0.05169323831796646,
          0.05184539780020714,
          0.0520806685090065,
          0.05170055478811264,
          0.05128311738371849,
          0.0513014942407608,
          0.05143801122903824,
          0.05171649530529976,
          0.05146476998925209,
          0.051934465765953064,
          0.05217832326889038,
          0.051678404211997986,
          0.05174776911735535,
          0.05147522687911987,
          0.05165242776274681,
          0.05125102028250694,
          0.05178157985210419,
          0.05119709670543671,
          0.051667846739292145,
          0.05179586261510849,
          0.05131005868315697,
          0.05147871747612953,
          0.05145353451371193,
          0.05147625133395195,
          0.051162417978048325,
          0.051140349358320236,
          0.05138545110821724,
          0.05126125365495682,
          0.05157523229718208,
          0.05208072438836098,
          0.05199044942855835,
          0.051636870950460434,
          0.05150988698005676,
          0.05145544931292534,
          0.05161517858505249,
          0.051410965621471405,
          0.05145418643951416,
          0.05176399275660515,
          0.05166924372315407,
          0.051479876041412354,
          0.05166410654783249,
          0.05133098363876343,
          0.0517745167016983,
          0.05151427909731865,
          0.051838699728250504,
          0.051486704498529434,
          0.0516691654920578,
          0.05142832174897194,
          0.05166172608733177,
          0.05178594961762428,
          0.05134356766939163,
          0.051296934485435486,
          0.051846764981746674,
          0.05218232050538063,
          0.05176323652267456,
          0.051339950412511826,
          0.05167797580361366,
          0.05160247161984444,
          0.05175318568944931,
          0.05159978196024895,
          0.05156690627336502,
          0.051371145993471146,
          0.051276881247758865,
          0.05199442431330681,
          0.05179038643836975,
          0.05126695707440376,
          0.05163272097706795,
          0.051387008279561996,
          0.05117752030491829,
          0.05133882910013199,
          0.05163191258907318,
          0.05159740895032883,
          0.05174451321363449,
          0.0514921098947525,
          0.051654789596796036,
          0.05156839266419411,
          0.05139685422182083,
          0.05170898139476776,
          0.051985494792461395,
          0.05149342492222786,
          0.051646240055561066,
          0.051318712532520294,
          0.051046065986156464,
          0.051666781306266785,
          0.05159032344818115,
          0.05099841207265854,
          0.05106889083981514,
          0.05139419436454773,
          0.0510336197912693,
          0.051695890724658966,
          0.05164700001478195,
          0.05163991078734398,
          0.05114548280835152,
          0.051453616470098495,
          0.05156543105840683,
          0.05163314938545227,
          0.051523275673389435,
          0.051804207265377045,
          0.051285792142152786,
          0.051302868872880936,
          0.051699332892894745,
          0.051651809364557266,
          0.05103639140725136,
          0.05097896233201027,
          0.051540557295084,
          0.05177213251590729,
          0.05193401873111725,
          0.05178855359554291,
          0.051637403666973114,
          0.05158897116780281,
          0.051502302289009094,
          0.05126229673624039,
          0.051308438181877136,
          0.0513935312628746,
          0.05160561949014664,
          0.05158295854926109,
          0.051278144121170044,
          0.05146467313170433,
          0.05164604261517525,
          0.05145366117358208,
          0.05176698416471481,
          0.0515778511762619,
          0.05120518431067467,
          0.05160801485180855,
          0.05149869993329048,
          0.05124391242861748,
          0.051926009356975555,
          0.051449764519929886,
          0.05168662220239639
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines",
         "name": "KL Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599
         ],
         "xaxis": "x2",
         "y": [
          62261,
          60993.34765625,
          52939.8046875,
          34494.4453125,
          13564.9208984375,
          12545.759765625,
          12437.5712890625,
          9895.7666015625,
          6215.587890625,
          5102.43017578125,
          3793.649658203125,
          2474.509521484375,
          2405.423828125,
          2966.138916015625,
          3176.881103515625,
          2827.08154296875,
          2303.741943359375,
          1675.875732421875,
          1098.319580078125,
          858.0807495117188,
          907.0927734375,
          967.9579467773438,
          863.4246826171875,
          653.2066650390625,
          496.48138427734375,
          457.4636535644531,
          451.411376953125,
          428.1786804199219,
          437.6556396484375,
          409.9320068359375,
          303.21075439453125,
          226.15231323242188,
          230.84286499023438,
          239.46875,
          220.2738800048828,
          198.2953338623047,
          170.914794921875,
          133.2894287109375,
          105.1502685546875,
          106.24537658691406,
          123.98973846435547,
          123.78239440917969,
          94.5916519165039,
          63.24099349975586,
          53.78809356689453,
          63.24518585205078,
          74.8599624633789,
          73.89183044433594,
          56.98551940917969,
          36.9407844543457,
          30.745031356811523,
          38.36137771606445,
          45.28559494018555,
          42.456050872802734,
          32.11648941040039,
          21.481019973754883,
          17.955320358276367,
          22.035966873168945,
          25.752426147460938,
          23.129390716552734,
          16.221242904663086,
          11.109164237976074,
          11.08718490600586,
          13.538844108581543,
          14.318072319030762,
          11.985682487487793,
          8.767236709594727,
          6.761359214782715,
          7.2368011474609375,
          8.831474304199219,
          8.998483657836914,
          6.775273323059082,
          4.836764812469482,
          4.916806697845459,
          5.777090549468994,
          6.134932994842529,
          5.474197864532471,
          4.027914047241211,
          3.478893518447876,
          4.171223163604736,
          4.603403091430664,
          4.184750556945801,
          3.3390369415283203,
          2.8205034732818604,
          3.156085729598999,
          3.6008410453796387,
          3.317471981048584,
          2.6745035648345947,
          2.4108738899230957,
          2.582054376602173,
          2.8187639713287354,
          2.606661081314087,
          2.23099946975708,
          2.1138482093811035,
          2.2477972507476807,
          2.3120431900024414,
          2.1522181034088135,
          1.960213303565979,
          1.8942803144454956,
          1.9556422233581543,
          1.981874942779541,
          1.8787035942077637,
          1.7627754211425781,
          1.7629204988479614,
          1.7465274333953857,
          1.772400140762329,
          1.698005199432373,
          1.622625708580017,
          1.6186881065368652,
          1.6127586364746094,
          1.5993397235870361,
          1.5446360111236572,
          1.5098278522491455,
          1.4907342195510864,
          1.466495156288147,
          1.4594175815582275,
          1.4360523223876953,
          1.3886890411376953,
          1.3746792078018188,
          1.3638571500778198,
          1.3388878107070923,
          1.3295230865478516,
          1.2990167140960693,
          1.293610692024231,
          1.2706362009048462,
          1.2615363597869873,
          1.2410423755645752,
          1.2251590490341187,
          1.206533432006836,
          1.19697105884552,
          1.1941813230514526,
          1.1554094552993774,
          1.149820327758789,
          1.1336486339569092,
          1.132357120513916,
          1.1207150220870972,
          1.0942538976669312,
          1.0872955322265625,
          1.0812844038009644,
          1.0703688859939575,
          1.0548954010009766,
          1.0316720008850098,
          1.0369709730148315,
          1.0175495147705078,
          1.0172336101531982,
          0.9883829355239868,
          0.9804811477661133,
          0.9790235161781311,
          0.9823862910270691,
          0.9585232734680176,
          0.9574084877967834,
          0.9434878826141357,
          0.9195513129234314,
          0.9301851987838745,
          0.9208260774612427,
          0.8961864113807678,
          0.8949365019798279,
          0.8866580724716187,
          0.8832058310508728,
          0.869417667388916,
          0.8600318431854248,
          0.8434346914291382,
          0.8561440706253052,
          0.8410282731056213,
          0.8328084945678711,
          0.8235251903533936,
          0.8008581399917603,
          0.806713879108429,
          0.7991965413093567,
          0.7988384962081909,
          0.788924515247345,
          0.772429883480072,
          0.7737981081008911,
          0.7753025889396667,
          0.761033833026886,
          0.7484866976737976,
          0.7504870891571045,
          0.7319547533988953,
          0.7330774664878845,
          0.7250996232032776,
          0.7156025767326355,
          0.7150648832321167,
          0.7172493934631348,
          0.7037844061851501,
          0.7001646757125854,
          0.6878082156181335,
          0.6818939447402954,
          0.6795575618743896,
          0.6783976554870605,
          0.6590683460235596,
          0.6549844741821289,
          0.6563147306442261,
          0.6528921127319336,
          0.6462453603744507,
          0.6425243020057678,
          0.628050684928894,
          0.6336329579353333,
          0.6224517822265625,
          0.6112384796142578,
          0.6182299852371216,
          0.6086927056312561,
          0.6079509258270264,
          0.6008725166320801,
          0.5979339480400085,
          0.5932319164276123,
          0.5834359526634216,
          0.5789839029312134,
          0.5727982521057129,
          0.5720977187156677,
          0.5663124322891235,
          0.5641120672225952,
          0.5543580055236816,
          0.5487606525421143,
          0.5518237352371216,
          0.5382062792778015,
          0.5382936596870422,
          0.531550943851471,
          0.5351424813270569,
          0.5266786217689514,
          0.5265397429466248,
          0.5229071974754333,
          0.5170259475708008,
          0.5161101818084717,
          0.509467601776123,
          0.5050090551376343,
          0.5060069561004639,
          0.5034889578819275,
          0.4955267906188965,
          0.49290889501571655,
          0.4910077750682831,
          0.4832090139389038,
          0.48140355944633484,
          0.4754180312156677,
          0.4710570275783539,
          0.46902695298194885,
          0.4639396369457245,
          0.4637640416622162,
          0.46142008900642395,
          0.45793378353118896,
          0.452953040599823,
          0.4457612931728363,
          0.44121477007865906,
          0.43861842155456543,
          0.4460572600364685,
          0.4348096251487732,
          0.4347172677516937,
          0.4320583939552307,
          0.429073691368103,
          0.4208707809448242,
          0.42363250255584717,
          0.41780391335487366,
          0.41780325770378113,
          0.4176003932952881,
          0.40603119134902954,
          0.4069472551345825,
          0.40874379873275757,
          0.4029015004634857,
          0.40066877007484436,
          0.39492368698120117,
          0.39950019121170044,
          0.3930959105491638,
          0.38388586044311523,
          0.39066532254219055,
          0.3818581700325012,
          0.3784330487251282,
          0.3765040338039398,
          0.38100358843803406,
          0.37320631742477417,
          0.3696015477180481,
          0.3751199543476105,
          0.36457595229148865,
          0.36579930782318115,
          0.36240848898887634,
          0.35464099049568176,
          0.3567923903465271,
          0.35405510663986206,
          0.35753488540649414,
          0.3511642515659332,
          0.35134002566337585,
          0.3467731177806854,
          0.34328654408454895,
          0.3420836925506592,
          0.34202298521995544,
          0.33752575516700745,
          0.33639442920684814,
          0.3349608778953552,
          0.3295045793056488,
          0.3286982476711273,
          0.3268572688102722,
          0.32413941621780396,
          0.3237817883491516,
          0.3225385844707489,
          0.3216336965560913,
          0.3197450637817383,
          0.3189813792705536,
          0.3172871470451355,
          0.31278425455093384,
          0.31103524565696716,
          0.3080432713031769,
          0.302823543548584,
          0.3011850118637085,
          0.29759490489959717,
          0.3013206720352173,
          0.2952609956264496,
          0.2953050136566162,
          0.29217270016670227,
          0.29037177562713623,
          0.2922566533088684,
          0.291168212890625,
          0.2837259769439697,
          0.2857521176338196,
          0.281062513589859,
          0.28444886207580566,
          0.28158038854599,
          0.2769314646720886,
          0.27993345260620117,
          0.27897337079048157,
          0.2756351828575134,
          0.2730376720428467,
          0.27060404419898987,
          0.2682170271873474,
          0.26721706986427307,
          0.26759615540504456,
          0.26603373885154724,
          0.2626507878303528,
          0.2623817026615143,
          0.25814732909202576,
          0.2613596022129059,
          0.26033756136894226,
          0.25704410672187805,
          0.25429612398147583,
          0.2507835328578949,
          0.2485266774892807,
          0.2500762939453125,
          0.24810409545898438,
          0.24799275398254395,
          0.24664798378944397,
          0.2408476173877716,
          0.2420700490474701,
          0.2425694316625595,
          0.24242964386940002,
          0.23824328184127808,
          0.23654738068580627,
          0.23731574416160583,
          0.23653657734394073,
          0.22969168424606323,
          0.23120562732219696,
          0.2307400405406952,
          0.22771793603897095,
          0.22726111114025116,
          0.22552181780338287,
          0.2253342866897583,
          0.22505563497543335,
          0.22465689480304718,
          0.22318419814109802,
          0.22089707851409912,
          0.2209610790014267,
          0.2205163538455963,
          0.21995891630649567,
          0.2121281772851944,
          0.21306808292865753,
          0.21139609813690186,
          0.20997865498065948,
          0.2105780392885208,
          0.21093271672725677,
          0.20806677639484406,
          0.2078130841255188,
          0.205216646194458,
          0.20483291149139404,
          0.20437243580818176,
          0.20444250106811523,
          0.20068815350532532,
          0.1999426931142807,
          0.20053134858608246,
          0.19716818630695343,
          0.1975305676460266,
          0.19671152532100677,
          0.19635269045829773,
          0.19173623621463776,
          0.1948888748884201,
          0.19775553047657013,
          0.19260041415691376,
          0.1904052346944809,
          0.19076396524906158,
          0.18921169638633728,
          0.1898621767759323,
          0.1872902512550354,
          0.18514522910118103,
          0.18590153753757477,
          0.1826106160879135,
          0.18399633467197418,
          0.1844216287136078,
          0.18067999184131622,
          0.1820562481880188,
          0.1785680204629898,
          0.17864571511745453,
          0.17672091722488403,
          0.1764782965183258,
          0.1767858862876892,
          0.17581599950790405,
          0.174390509724617,
          0.17421375215053558,
          0.17190180718898773,
          0.17128702998161316,
          0.16944529116153717,
          0.16855750977993011,
          0.16664470732212067,
          0.1673017144203186,
          0.1645253747701645,
          0.1666719615459442,
          0.16573624312877655,
          0.16374820470809937,
          0.16237278282642365,
          0.16622979938983917,
          0.1645178198814392,
          0.16354791820049286,
          0.15972910821437836,
          0.1615428775548935,
          0.15969295799732208,
          0.15659157931804657,
          0.157454714179039,
          0.15635137259960175,
          0.15536370873451233,
          0.15610167384147644,
          0.15573802590370178,
          0.15466512739658356,
          0.1526252031326294,
          0.1518825888633728,
          0.1533123254776001,
          0.1513773649930954,
          0.1497785598039627,
          0.1494763195514679,
          0.1495601385831833,
          0.14764006435871124,
          0.14676563441753387,
          0.14652954041957855,
          0.1468091607093811,
          0.1463310271501541,
          0.14482957124710083,
          0.14297515153884888,
          0.1426677256822586,
          0.14186899363994598,
          0.1403646618127823,
          0.1425257921218872,
          0.13896287977695465,
          0.1413973867893219,
          0.1396225392818451,
          0.14006389677524567,
          0.13887974619865417,
          0.13646021485328674,
          0.13819509744644165,
          0.13524547219276428,
          0.13700217008590698,
          0.1333397477865219,
          0.13567934930324554,
          0.13381965458393097,
          0.13122215867042542,
          0.13128617405891418,
          0.13098935782909393,
          0.13056056201457977,
          0.1310015171766281,
          0.12999379634857178,
          0.12978193163871765,
          0.1279444396495819,
          0.12783025205135345,
          0.12655198574066162,
          0.12645067274570465,
          0.12353984266519547,
          0.12520171701908112,
          0.12419556081295013,
          0.1243760883808136,
          0.12185881286859512,
          0.12362628430128098,
          0.12296688556671143,
          0.1217254176735878,
          0.11953546106815338,
          0.1192205622792244,
          0.12110929191112518,
          0.12120926380157471,
          0.11945953965187073,
          0.12057177722454071,
          0.12123868614435196,
          0.11859282851219177,
          0.11775333434343338,
          0.11713167279958725,
          0.11655682325363159,
          0.11394817382097244,
          0.11544221639633179,
          0.11491077393293381,
          0.11592031270265579,
          0.11415200680494308,
          0.11491064727306366,
          0.11292614787817001,
          0.11321920901536942,
          0.11329605430364609,
          0.11113151907920837,
          0.10967173427343369,
          0.11157433688640594,
          0.10979296267032623,
          0.10885641723871231,
          0.10939011722803116,
          0.11064469069242477,
          0.10891398787498474,
          0.10724571347236633,
          0.10842537879943848,
          0.10632352530956268,
          0.10632538050413132,
          0.10513494908809662,
          0.10492829233407974,
          0.10437770932912827,
          0.10488804429769516,
          0.10322917252779007,
          0.10117980092763901,
          0.10245092958211899,
          0.10242535173892975,
          0.10311281681060791,
          0.10178092122077942,
          0.10149906575679779,
          0.09970720112323761,
          0.10103634744882584,
          0.10087992995977402,
          0.10061639547348022,
          0.09954162687063217,
          0.09823493659496307,
          0.0993015393614769,
          0.09835442155599594,
          0.09822821617126465,
          0.09787642955780029,
          0.09768693894147873,
          0.09645413607358932,
          0.09547743201255798,
          0.0954369381070137,
          0.093864805996418,
          0.09408720582723618,
          0.09523837268352509,
          0.09498005360364914,
          0.09228286147117615,
          0.09392403811216354,
          0.0922117680311203,
          0.09165674448013306,
          0.09177816659212112,
          0.09136009216308594,
          0.09099084138870239,
          0.08953602612018585,
          0.09015706926584244,
          0.08940883725881577,
          0.08986259251832962,
          0.08911605179309845,
          0.08876565843820572,
          0.08896061033010483,
          0.08909451216459274,
          0.08792973309755325,
          0.08745674788951874,
          0.08720337599515915,
          0.08564434200525284,
          0.08637681603431702,
          0.08477562665939331,
          0.08506336808204651,
          0.08507828414440155,
          0.08429892361164093,
          0.08469673991203308,
          0.08484587073326111,
          0.08530858904123306,
          0.08412580192089081,
          0.08337877690792084,
          0.08427140861749649,
          0.08277823030948639,
          0.08398190140724182,
          0.08231120556592941,
          0.08170120418071747,
          0.0805843397974968,
          0.08186972141265869,
          0.08119886368513107,
          0.0804772675037384,
          0.07914616912603378,
          0.08110684901475906,
          0.07937151938676834,
          0.07961603999137878,
          0.07886739820241928,
          0.07799600064754486,
          0.07827731966972351,
          0.07752054929733276,
          0.07653466612100601,
          0.07783372700214386,
          0.0761122927069664,
          0.07693874835968018,
          0.0769813284277916,
          0.07651413232088089,
          0.07595591992139816,
          0.07569200545549393,
          0.07564198225736618,
          0.07544989138841629,
          0.07545290142297745,
          0.07316949218511581,
          0.07293372601270676,
          0.07307878136634827,
          0.07349559664726257,
          0.07404469698667526,
          0.07273820042610168,
          0.07258640974760056
         ],
         "yaxis": "y2"
        },
        {
         "mode": "lines",
         "name": "Total Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599
         ],
         "xaxis": "x3",
         "y": [
          62268,
          60996.7890625,
          52941.578125,
          34495.44140625,
          13565.3935546875,
          12546.2822265625,
          12439.0673828125,
          9897.3134765625,
          6217.4716796875,
          5104.14404296875,
          3794.727783203125,
          2476.106689453125,
          2406.44482421875,
          2966.736328125,
          3177.66796875,
          2827.766357421875,
          2304.103759765625,
          1676.2421875,
          1098.6298828125,
          858.5911254882812,
          907.720458984375,
          968.408447265625,
          863.6632080078125,
          653.46240234375,
          496.662841796875,
          457.6142883300781,
          451.6209716796875,
          428.59063720703125,
          438.0282897949219,
          410.109130859375,
          303.4403991699219,
          226.4426727294922,
          231.17965698242188,
          239.69488525390625,
          220.49578857421875,
          198.53518676757812,
          171.08644104003906,
          133.45120239257812,
          105.39884185791016,
          106.5364761352539,
          124.22747802734375,
          123.98011779785156,
          94.84979248046875,
          63.540565490722656,
          54.29593276977539,
          63.84579086303711,
          75.2355728149414,
          74.17337036132812,
          57.3699836730957,
          37.391395568847656,
          31.356212615966797,
          39.01099395751953,
          45.688270568847656,
          42.83399963378906,
          32.465877532958984,
          21.738435745239258,
          18.256567001342773,
          22.411937713623047,
          26.02396011352539,
          23.309307098388672,
          16.44378089904785,
          11.353572845458984,
          11.326980590820312,
          13.751484870910645,
          14.483829498291016,
          12.109269142150879,
          8.891653060913086,
          6.912445545196533,
          7.413248062133789,
          9.002957344055176,
          9.141289710998535,
          6.8917317390441895,
          4.953975200653076,
          5.047945976257324,
          5.913871765136719,
          6.262414932250977,
          5.576145648956299,
          4.1164069175720215,
          3.57580304145813,
          4.278151988983154,
          4.707584381103516,
          4.2782721519470215,
          3.4267640113830566,
          2.9062423706054688,
          3.2375290393829346,
          3.6773433685302734,
          3.391244888305664,
          2.749711513519287,
          2.488959789276123,
          2.6598105430603027,
          2.8921165466308594,
          2.6750407218933105,
          2.2978713512420654,
          2.1820528507232666,
          2.3164966106414795,
          2.3796916007995605,
          2.216167688369751,
          2.021568775177002,
          1.955446481704712,
          2.0171642303466797,
          2.0443954467773438,
          1.9404776096343994,
          1.822513461112976,
          1.8220641613006592,
          1.8045005798339844,
          1.8307881355285645,
          1.7571316957473755,
          1.681480050086975,
          1.6778101921081543,
          1.6707128286361694,
          1.6562676429748535,
          1.6011180877685547,
          1.5664383172988892,
          1.5474025011062622,
          1.522849440574646,
          1.5160434246063232,
          1.4923471212387085,
          1.443720817565918,
          1.43012535572052,
          1.4192824363708496,
          1.394277572631836,
          1.385392189025879,
          1.3543086051940918,
          1.3485463857650757,
          1.325537919998169,
          1.3165076971054077,
          1.2962039709091187,
          1.279979944229126,
          1.2616024017333984,
          1.2514872550964355,
          1.2488360404968262,
          1.2101268768310547,
          1.2038863897323608,
          1.1874284744262695,
          1.1863644123077393,
          1.1746232509613037,
          1.1482386589050293,
          1.1415112018585205,
          1.1352919340133667,
          1.124177098274231,
          1.1084847450256348,
          1.085282802581787,
          1.0905869007110596,
          1.071016550064087,
          1.0714144706726074,
          1.0418100357055664,
          1.033613920211792,
          1.0324029922485352,
          1.036623477935791,
          1.012570858001709,
          1.01150381565094,
          0.9973891377449036,
          0.9724918603897095,
          0.9834567308425903,
          0.9742723107337952,
          0.9493486881256104,
          0.948250412940979,
          0.939664363861084,
          0.936657726764679,
          0.9223242998123169,
          0.9131526947021484,
          0.8964189291000366,
          0.909318208694458,
          0.894216775894165,
          0.8859342336654663,
          0.8765040040016174,
          0.8538283109664917,
          0.8598897457122803,
          0.8523787260055542,
          0.8520419597625732,
          0.8422110676765442,
          0.8252408504486084,
          0.8268216848373413,
          0.828299880027771,
          0.8140904903411865,
          0.8013675212860107,
          0.8038018941879272,
          0.7846397161483765,
          0.7859799265861511,
          0.7776148319244385,
          0.768586277961731,
          0.7678929567337036,
          0.7706205248832703,
          0.7568132877349854,
          0.7530949711799622,
          0.7406665086746216,
          0.734605610370636,
          0.732438862323761,
          0.7313064336776733,
          0.7113057374954224,
          0.7073034048080444,
          0.7094414830207825,
          0.7059406638145447,
          0.6989752650260925,
          0.6949241161346436,
          0.6809272766113281,
          0.6866161823272705,
          0.6751036643981934,
          0.6632363796234131,
          0.6712440252304077,
          0.6615273356437683,
          0.6607040762901306,
          0.6532938480377197,
          0.6506538391113281,
          0.6461274027824402,
          0.6361758708953857,
          0.6314747929573059,
          0.6253201365470886,
          0.6245413422584534,
          0.6186898946762085,
          0.6166846752166748,
          0.606718897819519,
          0.6011865139007568,
          0.6046109199523926,
          0.590390145778656,
          0.5903276801109314,
          0.5837782025337219,
          0.5879581570625305,
          0.5789702534675598,
          0.5790911912918091,
          0.5753082633018494,
          0.5695527195930481,
          0.5686934590339661,
          0.5620476007461548,
          0.5572329163551331,
          0.5583194494247437,
          0.5563158988952637,
          0.547703206539154,
          0.5455767512321472,
          0.5434744954109192,
          0.5356240272521973,
          0.5335177183151245,
          0.5277305245399475,
          0.5229955911636353,
          0.5215579867362976,
          0.515994131565094,
          0.5158659815788269,
          0.5141387581825256,
          0.5103899240493774,
          0.5054757595062256,
          0.4976569712162018,
          0.493242472410202,
          0.4908598065376282,
          0.49821123480796814,
          0.4866911470890045,
          0.4867851436138153,
          0.4844294786453247,
          0.48152434825897217,
          0.4731330871582031,
          0.47597789764404297,
          0.47012171149253845,
          0.4704357385635376,
          0.4702652096748352,
          0.45760852098464966,
          0.45929598808288574,
          0.4607871472835541,
          0.45498380064964294,
          0.4526670277118683,
          0.44688719511032104,
          0.451618492603302,
          0.4455382525920868,
          0.4360274076461792,
          0.4432917833328247,
          0.43393847346305847,
          0.4302535355091095,
          0.42811936140060425,
          0.4334815740585327,
          0.4250352382659912,
          0.42181405425071716,
          0.42780667543411255,
          0.41654762625694275,
          0.4178120791912079,
          0.4144305884838104,
          0.4067828357219696,
          0.40861061215400696,
          0.4060610830783844,
          0.4096975326538086,
          0.40296265482902527,
          0.40360864996910095,
          0.399040162563324,
          0.3950493335723877,
          0.3941494822502136,
          0.39400431513786316,
          0.38941800594329834,
          0.3885887861251831,
          0.3864426612854004,
          0.3817875385284424,
          0.38061144948005676,
          0.3788650333881378,
          0.37591516971588135,
          0.37590712308883667,
          0.3744441568851471,
          0.37406009435653687,
          0.3721029758453369,
          0.37094971537590027,
          0.36957675218582153,
          0.36470645666122437,
          0.36326342821121216,
          0.36012735962867737,
          0.35452181100845337,
          0.35336995124816895,
          0.3491378724575043,
          0.3531756103038788,
          0.3467794954776764,
          0.34690332412719727,
          0.3437053859233856,
          0.34211236238479614,
          0.34417831897735596,
          0.3431633710861206,
          0.33552977442741394,
          0.3371586501598358,
          0.3327636420726776,
          0.33671730756759644,
          0.33342841267585754,
          0.3287467956542969,
          0.33168768882751465,
          0.3308231830596924,
          0.32780033349990845,
          0.32528796792030334,
          0.32218411564826965,
          0.32028719782829285,
          0.3188222348690033,
          0.31963256001472473,
          0.3179209232330322,
          0.314287930727005,
          0.31420382857322693,
          0.3096200227737427,
          0.3138686418533325,
          0.31247478723526,
          0.30882135033607483,
          0.3059206008911133,
          0.3029014468193054,
          0.3007698953151703,
          0.30181434750556946,
          0.3001643121242523,
          0.29986080527305603,
          0.2984749972820282,
          0.2923826575279236,
          0.2935543656349182,
          0.2940492331981659,
          0.2946631610393524,
          0.29001283645629883,
          0.2882694900035858,
          0.2890915274620056,
          0.2884421944618225,
          0.28098616003990173,
          0.28239646553993225,
          0.2826339304447174,
          0.2792486846446991,
          0.2790490686893463,
          0.2772921323776245,
          0.2770148515701294,
          0.2765887379646301,
          0.27638721466064453,
          0.27494198083877563,
          0.2725309133529663,
          0.273113876581192,
          0.272231787443161,
          0.27205806970596313,
          0.26370522379875183,
          0.2646368145942688,
          0.263260155916214,
          0.26160532236099243,
          0.262275367975235,
          0.2627033591270447,
          0.25973790884017944,
          0.25941330194473267,
          0.2566496431827545,
          0.256527304649353,
          0.2560689151287079,
          0.2562935948371887,
          0.25244659185409546,
          0.2514726221561432,
          0.252226859331131,
          0.24847371876239777,
          0.24941833317279816,
          0.2486272156238556,
          0.24821138381958008,
          0.24334804713726044,
          0.24650712311267853,
          0.24970971047878265,
          0.24427999556064606,
          0.24212509393692017,
          0.24294579029083252,
          0.24120035767555237,
          0.24180999398231506,
          0.23878422379493713,
          0.2368052452802658,
          0.23782333731651306,
          0.23439449071884155,
          0.23576098680496216,
          0.23594188690185547,
          0.23260515928268433,
          0.23406308889389038,
          0.22984234988689423,
          0.23053118586540222,
          0.22820493578910828,
          0.22837620973587036,
          0.22874011099338531,
          0.22743819653987885,
          0.22578972578048706,
          0.22612206637859344,
          0.223703533411026,
          0.2231762409210205,
          0.2209954708814621,
          0.22019562125205994,
          0.21823334693908691,
          0.2187739759683609,
          0.216170072555542,
          0.2184130847454071,
          0.2172006070613861,
          0.215593159198761,
          0.21366770565509796,
          0.21843445301055908,
          0.216367706656456,
          0.2153574824333191,
          0.21128828823566437,
          0.21345938742160797,
          0.21132433414459229,
          0.2082250714302063,
          0.20896676182746887,
          0.20792046189308167,
          0.2070695161819458,
          0.20829150080680847,
          0.2074713110923767,
          0.20645037293434143,
          0.20446789264678955,
          0.20354947447776794,
          0.20525392889976501,
          0.20315197110176086,
          0.20155927538871765,
          0.20081104338169098,
          0.20155978202819824,
          0.19899436831474304,
          0.19820979237556458,
          0.19823434948921204,
          0.19851373136043549,
          0.19833527505397797,
          0.19657166302204132,
          0.19493040442466736,
          0.1940353661775589,
          0.19380615651607513,
          0.192023366689682,
          0.19450698792934418,
          0.19060848653316498,
          0.1932184249162674,
          0.1912267506122589,
          0.19244681298732758,
          0.1907714605331421,
          0.18804487586021423,
          0.18985797464847565,
          0.18662214279174805,
          0.18851470947265625,
          0.18531666696071625,
          0.18782003223896027,
          0.18521994352340698,
          0.18298450112342834,
          0.18247084319591522,
          0.1823873221874237,
          0.1818777620792389,
          0.18269963562488556,
          0.1813199818134308,
          0.18149806559085846,
          0.1794613152742386,
          0.17957663536071777,
          0.1776949167251587,
          0.17770834267139435,
          0.17483384907245636,
          0.17693579196929932,
          0.17574916779994965,
          0.1758129596710205,
          0.17355205118656158,
          0.17547167837619781,
          0.17504754662513733,
          0.17342597246170044,
          0.17081858217716217,
          0.1705220639705658,
          0.17254731059074402,
          0.17292575538158417,
          0.17092430591583252,
          0.17250624299049377,
          0.17341700196266174,
          0.17027123272418976,
          0.16950109601020813,
          0.16860690712928772,
          0.1682092547416687,
          0.1651991903781891,
          0.16722379624843597,
          0.16610786318778992,
          0.16758815944194794,
          0.16594786942005157,
          0.16622070968151093,
          0.16440486907958984,
          0.16467274725437164,
          0.16477230191230774,
          0.162293940782547,
          0.16081207990646362,
          0.16295978426933289,
          0.16105422377586365,
          0.1604316532611847,
          0.16147084534168243,
          0.16263514757156372,
          0.16055086255073547,
          0.1587556004524231,
          0.1598808318376541,
          0.15793870389461517,
          0.15773634612560272,
          0.15658913552761078,
          0.1566922813653946,
          0.15604695677757263,
          0.1563679277896881,
          0.15489327907562256,
          0.15251079201698303,
          0.1542254388332367,
          0.1539396345615387,
          0.15495151281356812,
          0.15326762199401855,
          0.1531682312488556,
          0.15113551914691925,
          0.1526980698108673,
          0.1526658833026886,
          0.15195995569229126,
          0.15083855390548706,
          0.15008169412612915,
          0.15148386359214783,
          0.1501176655292511,
          0.14956817030906677,
          0.14955440163612366,
          0.14928941428661346,
          0.14820732176303864,
          0.14707721769809723,
          0.14700384438037872,
          0.14523595571517944,
          0.14536409080028534,
          0.1472328007221222,
          0.1467704474925995,
          0.1435498148202896,
          0.1455567628145218,
          0.1435987800359726,
          0.14283426105976105,
          0.1431169956922531,
          0.14299200475215912,
          0.14258825778961182,
          0.14128053188323975,
          0.14164918661117554,
          0.1410636305809021,
          0.14143098890781403,
          0.14051291346549988,
          0.14047464728355408,
          0.14094610512256622,
          0.1405879408121109,
          0.13957597315311432,
          0.13877546787261963,
          0.1382494419813156,
          0.13731113076210022,
          0.13796713948249817,
          0.13577404618263245,
          0.13613225519657135,
          0.13647247850894928,
          0.13533253967761993,
          0.13639262318611145,
          0.13649287819862366,
          0.13694849610328674,
          0.13527128100395203,
          0.13483239710330963,
          0.13583683967590332,
          0.13441137969493866,
          0.13550516963005066,
          0.13411541283130646,
          0.13298699259757996,
          0.13188721239566803,
          0.13356906175613403,
          0.13285067677497864,
          0.13151365518569946,
          0.13012513518333435,
          0.13264741003513336,
          0.13114365935325623,
          0.13155005872249603,
          0.1306559443473816,
          0.12963339686393738,
          0.12986628711223602,
          0.12902285158634186,
          0.1277969628572464,
          0.129142165184021,
          0.127505823969841,
          0.12854436039924622,
          0.1285642832517624,
          0.12779226899147034,
          0.1274205893278122,
          0.12733805179595947,
          0.12709563970565796,
          0.1272168755531311,
          0.12703076004981995,
          0.12437467277050018,
          0.1245417445898056,
          0.12457747757434845,
          0.12473951280117035,
          0.12597070634365082,
          0.12418796122074127,
          0.12427303194999695
         ],
         "yaxis": "y3"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Reconstruction Loss",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "KL Loss",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Total Loss",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.45,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 900,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Beta-VAE Training Losses"
        },
        "width": 900,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Iterations"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Iterations"
         }
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iterations"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.55,
          1
         ],
         "type": "log"
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.55,
          1
         ],
         "type": "log"
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          0.45
         ],
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = pd.DataFrame(trainer.pre_train_loss_history)\n",
    "\n",
    "# Create a subplot figure with 1x2 grid for individual losses, and a second row spanning the entire width for total loss\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\"Reconstruction Loss\", \"KL Loss\", \"Total Loss\"),\n",
    "    specs=[[{'type': 'scatter'}, {'type': 'scatter'}],\n",
    "           [{'colspan': 2, 'type': 'scatter'}, None]],\n",
    "    vertical_spacing=0.1,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Add traces for individual losses\n",
    "fig.add_trace(go.Scatter(x=loss_history.index, y=loss_history[\"Reconstruction Loss\"], mode=\"lines\", name=\"Reconstruction Loss\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=loss_history.index, y=loss_history[\"KL Loss\"], mode=\"lines\", name=\"KL Loss\"), row=1, col=2)\n",
    "\n",
    "# Add a trace for the total loss spanning the entire second row\n",
    "fig.add_trace(go.Scatter(x=loss_history.index, y=loss_history[\"Total Loss\"], mode=\"lines\", name=\"Total Loss\"), row=2, col=1)\n",
    "\n",
    "# Update the layout to include 'Iterations' as the x-axis name for each subplot\n",
    "fig.update_xaxes(title_text=\"Iterations\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Iterations\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Iterations\", row=2, col=1)  # The third row spans two columns\n",
    "\n",
    "fig.update_yaxes(type=\"log\", row=1, col=1)\n",
    "fig.update_yaxes(type=\"log\", row=1, col=2)\n",
    "fig.update_yaxes(type=\"log\", row=2, col=1)  # The third row spans two columns\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(height=900, width=900, title_text=\"Beta-VAE Training Losses\", showlegend=False)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
